\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {2.1}{\ignorespaces Visual comparison of common kernel functions and their effect on Gaussian process priors. Each column shows the kernel shape $k(x, x')$, samples from the corresponding GP prior, and a summary of the structure it imposes. All kernels were evaluated using a lengthscale parameter $\ell = 1$ (except where noted). For the Matern kernel, $\nu = 0.5$; Laplace kernel, $\gamma = 6$; Rational Quadratic kernel, $\alpha = 0.25$; and Periodic kernel, period $p = 2$. Detailed formula and graphs for each kernel are provided in the appendix \ref {appendix:B}. }}{11}{table.caption.14}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {3.1}{\ignorespaces Summary of the Gaussian Process Regression models evaluated. ''All kernels'' refers to the RBF, Matern, Rational Quadratic, ExpSine Squared and the Laplace kernels discussed in Section~\ref {sec: Kernels}. In the ''All Kernels'' case the method was ran for every kernel.}}{22}{table.caption.32}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {4.1}{\ignorespaces This table contains the optimized hyperparameters for the final 8 GPR models. $\sigma _{f,1}^2$ and $\sigma _{f,2}^2$ represent the constant factor the each kernel is multiplied by and is referred to as the signal variance since it controls the amplitute of the resulting model. The Length scales are the corresponding length scales for each dimension and then $\sigma _n^2$ is the optimised noise hyper-parameter introduced by the \textsc {WhiteKernel}. Mat and Lap refer to the Matern and Laplacian kernel.}}{27}{table.caption.42}%
\contentsline {table}{\numberline {4.2}{\ignorespaces Comparison of regression performance metrics for the RBFMat model using a pointwise (MAP) solution versus the MCMC marginalised model. The MCMC model reflects increased uncertainty, resulting in reduced fit scores but a more robust and probabilistically faithful representation.}}{29}{table.caption.45}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {1}{\ignorespaces Final Model Rankings after training on 90\% and testing on 10\%}}{39}{table.caption.57}%
\contentsline {table}{\numberline {2}{\ignorespaces All 32 Model Rankings from CV}}{39}{table.caption.58}%
\contentsline {table}{\numberline {3}{\ignorespaces Comparison of different performance metrics used in evaluating models. RMSE, \( R^2 \), FOM, and the Pearson Coefficient are included. MAE is similar to RMSE but without squaring errors. Adjusted \( R^2 \) accounts for the number of predictors and is slightly modified from \( R^2 \). The actual metrics for each graph are: RMSE = 0.2, \( R^2 \) = 0.6, FOM = 1.09, Pearson correlation = 0.8.}}{40}{table.caption.59}%
