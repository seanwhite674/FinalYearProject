\documentclass{article}
%\usepackage[backend=biber,style=numeric]{biblatex} % Use biblatex with numeric citations
\usepackage{cite}
\usepackage{hyperref}  
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{subcaption}
\usepackage{ulem} 
\usepackage{comment}
\usepackage{amsmath}
\usepackage{float} 
\setlength{\parskip}{0.5em}
% Enable clickable references
\usepackage{graphicx}        
\usepackage{array}
\usepackage[margin=0.8in]{geometry}
\usepackage{adjustbox}
\usepackage{makecell}
\usepackage{amssymb}
\newcommand{\Sarp}[1]{{\textcolor{blue}{{SA: #1}} }}
\newcommand{\Sean}[1]{{\textcolor{red}{{Sean: #1}} }}


\usepackage{cite} % For citation handling
\bibliographystyle{acm}  
 % without .bib extension


\title{FYP }
\author{Sean White}
\date{January 2025}

\begin{document}

\maketitle

\section{GR intro}


Gravitational waves (GWs) are small fluctuations of spacetime that propagate at the speed of light. As described in \textit{Gravitational Waves, Vol. 1} by Maggiore~\cite[p.~29]{GRbook},
\begin{quote}
“In this setting, the definition of GWs is relatively clear: the background spacetime is flat, and the small fluctuations around it have been called ‘gravitational waves’. The term ‘waves’ is justified by the fact that, in a suitable gauge, \(h_{\mu\nu}\) indeed satisfies a wave equation.”
\end{quote}
This is formully appproached by expanding the Einstein equations around the flat Minkowski metric $\eta _{a b}$ 
\begin{equation}
g_{ab} = \eta_{ab} + \epsilon\,h_{ab},
\qquad
\epsilon \ll 1,
\qquad
\eta_{ab} = \mathrm{diag}(-1, 1, 1, 1),
\end{equation}
where \(h_{ab}\) represents the perturbation. In the linearized regime of general relativity, the Einstein field equations simplify to
\begin{equation}
    \Box \bar{h}_{ab} = \frac{-16\pi G}{c^4} T_{\mu \nu}.
\end{equation}
However we are interested in this equation outside of the source (i.e $T_{\mu \nu} =0$) therefore we are left with 
\begin{equation}
    \Box \bar{h}_{ab} = 0, 
\end{equation}
with \(\Box\) the d’Alembertian operator in flat spacetime.

\noindent
Although \(h_{ab}\) initially has 10 independent components (as a symmetric $4 \times 4$ tensor), 8 of these correspond to gauge freedom and constraints. After imposing the Lorentz and transverse-traceless (TT) gauge conditions, only two physical degrees of freedom remain: the plus (\(h_+\)) and cross (\(h_\times\)) polarizations~\cite[Sec.~1.2]{GRbook}.

\noindent
This wave equation admits plane-wave solutions. For a wave propagating in the \(z\)-direction, the TT-gauge form of the perturbation is:
\begin{equation}
h_{ab}^{(\mathrm{TT})} \;\propto\;
\begin{pmatrix}
0 & 0 & 0 & 0 \\
0 & h_+ & h_\times & 0 \\
0 & h_\times & -h_+ & 0 \\
0 & 0 & 0 & 0
\end{pmatrix}
e^{i(kz-\omega t)}.
\end{equation}

% One can visualize \(h_+\) and \(h_\times\) by studying how freely-falling test particles in
% the \(x\)--\(y\) plane move under each polarization: \(h_+\) distorts along \(x\) and \(y\)
% axes, whereas \(h_\times\) acts on diagonals at \(\pm45^\circ\). In realistic astrophysical
% sources, both polarizations are generally present.

\subsection*{Leading-Order Power Emission by Gravitational Waves}
\label{subsec:GW_power}

In general, the power emitted by a radiative field can be expressed schematically as
\begin{equation}
    \dot{E} = \sum_{\ell = 0}^{\infty} \left\langle \left| \left( \frac{\partial}{\partial t} \right)^{\ell+1} P_\ell(t) \right| \right\rangle.
\end{equation}
Here, \(P_{\ell}(t)\) represents the multipole moments of the radiating source. The \(\ell = 0\) term corresponds to the monopole moment, which in the gravitational case is the total mass of the system. Assuming mass is conserved, this term vanishes.

\noindent
The \(\ell = 1\) term represents the dipole moment, which is also zero in the gravitational case due to conservation of linear momentum. Therefore, the leading-order contribution to gravitational-wave emission arises from the \(\ell = 2\) term, known as the quadrupole.

\noindent
This gives the leading-order expression for the power emitted in gravitational waves:
\begin{equation}
\dot{E} = \frac{G}{5\,c^5}
\left\langle \dddot{Q}_{ij}\,\dddot{Q}^{\,ij} \right\rangle,
\end{equation}
where \(Q_{ij}\) is the mass quadrupole moment of the source. It is related to the mass moment \(M_{ij}\) by
\begin{equation}
Q_{ij} := M_{ij} - \frac{1}{3} \delta_{ij} M^k_{\ k},
\end{equation}
where \(M_{ij} = \int d^3x\, T^{00}(x^i x^j)\).



\subsection*{A Simple Two-Mass Inspiral Model}
\label{subsec:two_mass_example}


\textcolor{red}{I took this example and summarised it from Sarp's notes, leads nicely to mismatch}

\noindent
We will now consider a generic problem as illistrated in \cite{intoGRSarp} where two point masses, \(m_1 \ge m_2\), are in a quasi-circular orbit of separation \(R\),
each at distances \(r_1\) and \(r_2\) from their common center of mass (CoM), with
\(R = r_1 + r_2\). We place the orbit in the \(x\)--\(y\) plane so that mass~1 moves on
\(\mathbf{x}_1(t) = r_1(\cos\Omega t,\,\sin\Omega t)\) and mass~2 on
\(\mathbf{x}_2(t) = r_2(\cos(\Omega t+\pi),\,\sin(\Omega t+\pi))\). The system’s orbital
frequency is \(\Omega\). A short calculation yields

\begin{equation}
    Q^{ij} = 4 \Omega^3 \left(m_1 r_1^2 + m_2 r_2^2\right)
    \begin{pmatrix}
    \sin(2\Omega t) & -\cos(2\Omega t) \\
    -\cos(2\Omega t) & -\sin(2\Omega t)
    \end{pmatrix}.
\end{equation}

\noindent
Introducing the reduced mass \(\mu = m_1 m_2/(m_1 + m_2)\), we get that the \(\ell=2\) power emission is

\begin{equation}\label{eq:E_dot_2}
\dot{E}_{\ell=2}
\;=\;
\frac{32}{5}\;\frac{G}{c^5}\;\Omega^6\,\mu^2\,R^4.
\end{equation}
Both \(\Omega\) and \(R\) are functions of time, but they evolve on a time scale much longer than the orbital time
scale and so when averaging over orbits we approximate them as constant. Applying Kepler’s law, \(\Omega^2 = GM/R^3\), and defining
\(\omega = 2\Omega\) as the GW frequency, we find
\begin{equation}
\dot{E}_{\ell=2}
\;=\;
\frac{32}{5}\;\frac{c^5}{G}
\left(\frac{G\,\mathcal{M}\;\omega}{2c^3}\right)^{10/3},
\quad
\text{where}
\quad
\mathcal{M} \;=\;\mu^{3/5}\,M^{2/5}
\end{equation}

\noindent
is known as the chirp mass where $M = m_1+m_2$ is the total mass.
\Sarp{You already kind of mention this below Eq.~\eqref{eq:E_dot_2}, so maybe provide slightly
more detail there.}

\noindent
\textcolor{red}{Could maybe mention radiation time scale being much smaller than orbit timescale but I don't see what this will add}

\noindent
$ \dot{E} $ represents the rate at which the system loses energy due to gravitational-wave emission.
This energy loss causes the binary orbit to shrink and the GW frequency to increase, 
with the chirp mass \(\mathcal{M}\) and frequency \(\omega\) capturing the key features of this inspiral.

\Sarp{From here you should go on to derive the form of a simple waveform in first time domain then frequency domain. Then you can go into the Mismatch}

\subsection*{Introducing the Waveform Mismatch}
\label{subsec:mismatch_intro}

This simplified two-mass, circular-orbit model captures the main physical components of the model such as the "chirp behaviour" (frequency of GW increasing as the orbit shrinks).
However we want to quantify how neglecting for example eccentricity of the orbit and the spin vectors of the masses
effects the waveform. We compare the simplified model to more complete waveforms using the waveform mismatch discussed in \cite{mismatch} and \cite{Ogpaper}. The mismatch between two signals is defined by
\begin{equation}
\label{eq:mismatch_def}
\mathcal{M} 
\;=\; 
1 \;-\;
\max_{\mathbf{\lambda_m}}
\frac{\langle h_{\text{simple}},\,h_{\text{accurate}}\rangle}
{\sqrt{\langle h_{\text{simple}},h_{\text{simple}}\rangle \,
\langle h_{\text{accurate}},h_{\text{accurate}}\rangle}},
\end{equation}
where \(\langle\cdot,\cdot\rangle\) is the inner product of both GW, and we maximise over a set of (intrinsic or extrinsic) model parameters \(\mathbf{\lambda_m}\).
A mismatch \(\mathcal{M}\ll 1\) indicates that our simple waveform
faithfully represents the physical signal, whereas larger mismatches highlight missing
physics (e.g. not circular orbit).




\section{Background}
\noindent
\textcolor{red}{This section was written ages ago and needs to be updated but I think could still be worth time
just going over how bayesian methods are currently used in GW and how making a mismatch predictor model may help
moving forward}

\noindent
The gold standard gravitational waveforms are obtained by numerical relativity simulations which involve directly solving Einstein's equations of general relativity which is massively computationally expensive. For this reason, only several thousand simulations are currently available. As a result, GW models rely on analytical or semi-analytical prescriptions that are calibrated to the numerical relativity simulations.

\par
\noindent
Each of these modelling approaches will incur a certain amount of error. We quantify this error as the mismatch between the "true" relativity simulation of a gravitational waveform and the analytic model for the waveform.The mismatch \cite{mismatch} varies between 0, signifying that the model and the true waveform are identical (up to an overall amplitude rescaling), and 1, meaning that the two are completely orthogonal. To begin we will delve into the methods used before to "combine" these models and then discuss the new proposed method. This will lead us onto my project.



\subsection*{Methods Used Before}

\subsubsection*{Standard Method}
We build a distribution based off of each of the 3 models. We then combine these distributions with equal weight to form a new total distribution from which we make our predictions from.

\subsubsection*{Evidence-Informed Method}
This method again builds a distribution based off of each of the 3 models. The distributions are then combined with weights which are determined by how well the model explains the observed data. From this new distribution predictions are made.

\subsection*{New Proposed Method: Numerical Relativity Informed Method }%\cite{hoy2024multi}}

\subsubsection*{Numerical Relativity Informed Method}
This method builds a distribution based off of each of the 3 models. Then the mismatch (difference between NR simulations and model) is calculated. We then find the ratios of the mismatch between each model, for example:
\[
\frac{\text{mismatch(model 1)}}{\text{mismatch(model 2)}}.
\]
A distribution using the ratio of mismatch of each model is built. This distribution is used to determine the probability of selecting each model in each region of the parameter space. Predictions are then made using the model selected in that region of the parameter space using the ratio distribution. This selection of the model is probabilistic and so the best model in a certain region may be chosen 85\% of the time and the second best 10\% etc etc.

\subsubsection*{Comparison}
The Standard Method and the Evidence-Informed Method build a distribution from which we make our predictions. The Numerical Relativity Informed method builds a distribution which helps us choose the best model in that parameter space ``most of the time"".

\noindent
Now the issue is that to calculate the mismatch for the models needed in the NR informed method, we must have gravitational waveforms generated from NR simulations. This as we discussed is a computationally expensive process and is often not feasible. The goal of my project is to build a Gaussian Process Regression model to model the mismatch of the model's to the NR simulations.

\begin{comment}
\newpage
A motivating example for this is outlined below.
\begin{figure}[h] 
    \centering
\includegraphics[width=1.1\textwidth]{Images/image.png} % 120% of text width
    %\caption{}
    %\label{fig:example-image}
\end{figure}

the black contours here represent a ratio of the mismatch between two different models for Gravitational Waveforms. Models used: model1: IMRPHENOMXPHM, model2: SEOBNRV5PHM, model3: IMRPHENOMTPHM. The left graph contours give the $\frac{mismatch(model1)}{mismatch(model2)}$ and right gives $\frac{mismatch(model3)}{mismatch(model2)}$. A mismatch ratio greater than 1 indicates that model 2 is closer to the NR simulations.
\end{comment}


\section{Gaussian Process Regression \textbf{GPR} Background}
\subsection{Introduction and Roadmap}

In the following subsections, I discuss the foundational concepts of Gaussian Process Regression \textbf{(GPR)} in four main steps:
\begin{enumerate}
    \item \textbf{Gaussian Processes:} Section~\ref{sec: GP_backgroound} introduces Gaussian Processes and explains how their priors and posteriors are constructed from finite sets of points.
    \item \textbf{Kernel Functions:} In Section~\ref{sec: Kernels}, I explore how kernels encode assumptions about smoothness, periodicity, and structural properties of the underlying function.
    \item \textbf{Noise Modeling:} Section~\ref{sec: Handlingnoise} covers several approaches for incorporating observational noise into the GP framework.
    \item \textbf{Hyperparameter Optimization:} Finally, in Section~\ref{sec: Hyper_parameters}, I discuss how kernel and noise hyperparameters influence the posterior and how they can be optimized to improve model performance.
\end{enumerate}

\subsection{Gaussian Proces Regression Background}
\label{sec: GP_backgroound}

\subsubsection*{Definition of a Gaussian Process}
\label{sec: Definition_of_GP}

A Gaussian Process \textbf{(GP)} defines a probabilistic model over all possible functions rather than assuming a single function to be true
\begin{equation}
f(X) \sim \mathcal{GP} (\mu(X), k(X, X')).
\label{eq: Initial_GP_distribution}
\end{equation}

\noindent
where \( \mu(X) \) is the mean function, specifying the expected function value at each \( X \):
\begin{equation}
    \mu(X) = {E}[f(X)],
    \label{eq: meandef}
\end{equation}

\noindent
 \( k(X, X') \) is the covariance function (kernel), encoding the relationships between function values at different points:
\begin{equation}
    k(X, X') = \text{Cov}(f(X), f(X')).
    \label{eq: kerneldef}
\end{equation}

\noindent
Since the input space is continuous, the GP represents an infinite-dimensional distribution. 
In practice, we approximate the process by evaluating the GP at a finite set of inputs.
These function values are then assumed to follow a multivariate normal Gaussian distribution.

\noindent
Mathematically, for a finite set of input points

\begin{equation}
X = \{X_1, X_2, \dots, X_n\},
\end{equation}
 the corresponding function values
\begin{equation}
f = \{f(X_1),f(X_2),...,f(X_n), \}
\end{equation}
follow a multivariate normal distribution
\begin{equation}
f \sim \mathcal{N}(\mu(X), K(X, X)).
\label{eq: Multivariate_distribution}
\end{equation}

\noindent
Each sample from this multivariate distribution represents a function evaluated at \( n \) different points.

\subsubsection*{The Prior Distribution}
\label{sec: prior_dist}

Before observing any data, we assume a joint Gaussian distribution over both training and test points. Let \( X \) denote training inputs and \( X_* \) test inputs. 
The joint prior over their function values is
\begin{equation}
\begin{bmatrix}
f(X) \\
f(X_*)
\end{bmatrix}
\sim \mathcal{N}
\left(
\begin{bmatrix}
\mu(X) \\
\mu(X_*)
\end{bmatrix},
\underbrace{
\begin{bmatrix}
K(X, X) & K(X, X_*) \\
K(X_*, X) & K(X_*, X_*)
\end{bmatrix}
}_{\mathcal{C} = \text{Covariance Matrix}}
\right).
\label{eq: Multivariate prior}
\end{equation}

\noindent
The corresponding joint probability density function (pdf) is given by:

\begin{equation}
    p(f, f_*) = \frac{1}{(2\pi)^{n/2} \sqrt{|\mathbf{C}|}} \exp\left( 
    - \frac{1}{2}\left(\begin{bmatrix}f \\f_*\end{bmatrix}-
    \begin{bmatrix}\mu(X) \\\mu(X_*)\end{bmatrix}\right)^T
    \mathbf{C}^{-1}\left(
    \begin{bmatrix}f \\f_*\end{bmatrix}-
    \begin{bmatrix}\mu(X) \\\mu(X_*)\end{bmatrix}\right)\right)
\end{equation}
    

\noindent
After accounting for the mean, the resulting distribution is entirely determined by its kernel function.
The kernel governs how the model generalizes to unseen data. There are many kernel choices, each encoding different structural assumptions about the function, such as smoothness, periodicity, or linearity.
In the next section we examine the different kernel choices available and the assumptions that each kernel encodes about our function structure, such as smoothness and periodicity.



\subsection{Kernel Functions}
\label{sec: Kernels}
The kernel function encodes our assumptions about the relationship between input points in a Gaussian Process (GP).
It defines the covariance between any two function values and thereby determines the smoothness, periodicity, or other properties of the functions drawn from the GP prior.
Fundamentally, kernels reflect the idea of *similarity*: input points \( x \) and \( x' \) that are close together are assumed to have highly correlated outputs \( f(x) \) and \( f(x') \),
while distant inputs are assumed to produce less correlated values. This notion of similarity, as emphasized in \cite[p.~79]{bible}, is central to how Gaussian processes learn from and generalize beyond training data.

\vspace{1em}
\noindent
In Figure~\ref{fig: samples_from_GP_prior}, we illustrate the effect of the kernel on the GP prior.
We draw three functions from the multivariate Gaussian prior defined in Equation~\ref{eq: Multivariate prior}, using a zero mean and an RBF kernel.
The first subplot shows these samples, while the second subplot visualizes the corresponding covariance matrix as a heatmap.
The matrix reveals that correlations are strongest when input points are close together (near the diagonal) and decay as the distance between inputs increases.
This is evident also from the samples as we can see nearby points often move in similar directions, while distant points diverge more significantly.

\vspace{1em}
\noindent
The final three subplots highlight how this distance-based correlation manifests in the joint distribution of pairs of function values.
For closely spaced inputs, such as \( (x, x') = (0, 0.1) \), the joint distribution of \( (f(0), f(0.1)) \) forms a narrow elliptical contour,
indicating strong correlation (approximately 0.9). As the distance increases, such as in the pairs \( (0, 0.5) \) and \( (0, 1) \), the ellipses widen, reflecting weaker correlation. This visualization reinforces the intuition that kernel functions govern how input proximity translates to output similarity.

\begin{figure}[H]
    \centering
        \includegraphics[width=\textwidth]{LatexPlots/1dplots/Kernel_background.png}
        \caption{Sampling from the GP prior with zero mean and an RBF kernel ($\ell = 0.5$, $\sigma_f^2 = 1$). 
        The first plot shows three sample functions drawn from the prior distribution. 
        The second plot visualizes the covariance matrix as a heatmap, revealing the strength of correlations between inputs.
        The final three subplots display joint distributions between selected input pairs, illustrating how output correlation diminishes with increasing input distance.}
    \label{fig: samples_from_GP_prior}
\end{figure}


\noindent
We have discussed how the kernel function encodes the covariance structure of the GP prior. This structure depends on the choice of kernel. According to [\cite{bible}]
kernels can be divided into two major sub-groups, stationary kernels and non-stationary kernels. 
Stationary kernels depend only on the relative (often radial) distance between inputs \(\|x - x'\|\) and are invariant to translations 
in the input domain. By contrast, non-stationary kernels depend explicitly on 
the absolute values of \(x\) and \(x'\), allowing the function’s properties—such as smoothness 
or amplitude—to vary across the domain. For more detailed discussion on building, combining, and customizing these kernels, 
see \cite{kernelcookbook} and \cite[Ch.~4]{bible}.

\bigskip

\noindent
In Table~\ref{tab:kernel-examples}, we provide an overview of several common kernel types, 
showing both their functional form and samples drawn from the corresponding GP priors. 
While each kernel imposes a distinct structural pattern on the functions—such as smoothness, periodicity, or linearity—they are all similarly influenced by shared hyperparameters like the lengthscale. 
In addition, many kernels include unique internal parameters that further shape the behaviour of the modeled functions. 
In the following subsections, we explore each of these kernels in detail and discuss the role of their associated hyperparameters.


\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{4} % Slightly tighter rows
    \setlength{\tabcolsep}{2pt} % Tighter columns
    \small % Smaller text to fit content

    \begin{tabular}{|>{\centering\arraybackslash}m{2cm}|*{6}{>{\centering\arraybackslash}m{2.3cm}|}} 
        \hline
        \textbf{Kernel name:} & \textbf{RBF (SE)} & \textbf{Rational Quadratic} & \textbf{Periodic} & \textbf{Matern} & \textbf{Laplace} & \textbf{Linear (Dot Product)} \\ 
        \hline
        \textbf{Plot of $k(x, x')$:} & 
        \adjustbox{valign=c}{\includegraphics[width=2.25cm]{LatexPlots/1dplots/Kernel_RBF_SE.png}} & 
        \adjustbox{valign=c}{\includegraphics[width=2.25cm]{LatexPlots/1dplots/Kernel_Rational_Quadratic.png}} & 
        \adjustbox{valign=c}{\includegraphics[width=2.25cm]{LatexPlots/1dplots/Kernel_Periodic.png}} & 
        \adjustbox{valign=c}{\includegraphics[width=2.25cm]{LatexPlots/1dplots/Kernel_Matern_nu05.png}} & 
        \adjustbox{valign=c}{\includegraphics[width=2.25cm]{LatexPlots/1dplots/Kernel_Laplace_Exponential.png}} & 
        \adjustbox{valign=c}{\includegraphics[width=2.25cm]{LatexPlots/1dplots/Kernel_Linear_Dot_Product.png}} \\ 
        \hline
        \textbf{GP Prior Samples:} & 
        \adjustbox{valign=c}{\includegraphics[width=2.25cm]{LatexPlots/1dplots/KernelSample_RBF_SE.png}} & 
        \adjustbox{valign=c}{\includegraphics[width=2.25cm]{LatexPlots/1dplots/KernelSample_Rational_Quadratic.png}} & 
        \adjustbox{valign=c}{\includegraphics[width=2.25cm]{LatexPlots/1dplots/KernelSample_Periodic.png}} & 
        \adjustbox{valign=c}{\includegraphics[width=2.25cm]{LatexPlots/1dplots/KernelSample_Matern_nu05.png}} & 
        \adjustbox{valign=c}{\includegraphics[width=2.25cm]{LatexPlots/1dplots/KernelSample_Laplace_Exponential.png}} & 
        \adjustbox{valign=c}{\includegraphics[width=2.25cm]{LatexPlots/1dplots/KernelSample_Linear_Dot_Product.png}} \\ 
        \hline
        \textbf{Key Hyperparameters} &
        $\ell$ (Lengthscale) &
        \shortstack{$\ell$ \\ $\alpha$ (Scale-mix)}&
        \shortstack{$\ell$ \\ $p$ (Period)} &
        \shortstack{$\ell$ \\ $\nu$ (Smoothness)} &
        $\gamma$ (Decay rate) &
        None or variance \\ 
        \hline
        \textbf{Structure type:} & 
        Local variation & 
        Multi-scale local variation & 
        Repeating structure & 
        Rough to smooth & 
        Rougher variation & 
        Linear functions \\ 
        \hline
    \end{tabular}
    \caption{
        Visual comparison of common kernel functions and their effect on Gaussian process priors. 
        Each column shows the kernel shape $k(x, x')$, samples from the corresponding GP prior, and a summary of the structure it imposes. 
        All kernels were evaluated using a lengthscale parameter $\ell = 1$ (except where noted). 
        For the Matern kernel, $\nu = 0.5$; Laplace kernel, $\gamma = 6$; Rational Quadratic kernel, $\alpha = 0.25$; and Periodic kernel, period $p = 2$.
        Detailed formula and graphs for each kernel are provided in the appendix \ref{appendix:B}.
        }
    \label{tab:kernel-examples}
\end{table}

\noindent
Note: In practice, we scale each kernel by a signal variance hyperparameter \(\sigma_f^2\), which governs the overall vertical variation in the function.
This scaling is applied consistently across all kernel types and is discussed further in the \ref{sec: Handlingnoise}.

\noindent
From Table~\ref{tab:kernel-examples}, we observe that the RBF, Rational Quadratic, Matern, Laplace, and Periodic kernels are all examples of stationary kernels (i.e depend on $|x-x'|$). 
Many of these—such as the RBF, Matern,Rational Quadratic and Laplace—exhibit "bell-shaped" structures: inputs \(x\) and \(x'\) that are close together yield high covariance,
which then decays as the distance \(\|x - x'\|\) increases.The Periodic kernel, while also stationary, has a unique structure. Instead of decaying monotonically with distance, 
it assigns high covariance to inputs that are separated by integer multiples of a fixed period \(p\). This leads to a repeating pattern of similarity,
making the kernel ideal for modeling functions which are periodic.

\noindent
We have examined multiple kernel types and their properties.We will now briefly visualise and examine the effect of the lengthscale hyperparameter and 
the signal variance hyperparameter on the GP prior. We will use the RBF kernel as an example, but the same principles apply to other kernels.



\begin{figure}[H]
    \centering
        \includegraphics[width=\textwidth]{LatexPlots/1dplots/Kernel_hyperparams.png}
        \caption{Sampling from the GP prior with mean 0 and covariance given by the RBF kernel.
        The first plot shows the effect of the lengthscale hyperparameter \(\ell\) on the GP prior.We fix the signal variance to 1.
        The second plot shows the effect of the signal variance hyperparameter \(\sigma_f^2\) on the GP prior.We fix the lengthscale to 0.5.}
    \label{fig: GPprior_hyperparams}
\end{figure}

\Sean{Comment on how each effects prior. Below is waffle from a while ago could potentially add}

\noindent
It is evident that the choice of kernel hyperparameters has a significant impact on the shape of the posterior mean and its uncertainty.
The RBF kernel with noise is given by:
\[
k(x, x') = \sigma_f^2 \exp\left( -\frac{(x - x')^2}{2\ell^2} \right) + \sigma_n^2 I,
\]
where $\ell$ controls the length scale, $\sigma_f^2$ represents the signal variance, and $\sigma_n^2$ denotes the noise level.

\noindent
From Figure~\ref{fig: GPprior_hyperparams}, we observe that a very small length scale $\ell \approx 0$ leads to overfitting:
the GP closely tracks the training data, with sharp fluctuations between points. This occurs because the kernel's covariance rapidly decays with distance,
making function values at different inputs nearly uncorrelated unless they are extremely close. As a result, the model fits the noise and exhibits a jagged appearance.
In contrast, a large length scale such as $\ell \approx 3$ causes over-smoothing. Distant points remain highly correlated,
producing a nearly linear mean function that fails to capture local variations. A moderate length scale ($\ell = 0.109$ in this case)
provides a balance—producing a smooth yet flexible mean function that captures trends in the data without overfitting.

\noindent
Regarding the signal variance $\sigma_f^2$, we find that a very small value (e.g., $\sigma_f^2 \approx 0.01$) causes the GP mean to flatten,
as the prior assumes the function to vary very little. As the variance increases (e.g., $\sigma_f^2 = 1.2$ and $\sigma_f^2 = 10$), the credible intervals widen slightly,
but the overall mean function shape remains relatively stable. This suggests that posterior inference over $\sigma_f^2$ may exhibit considerable uncertainty, a point we explore further using MCMC sampling in Section \ref{sec: MCMC}.


\noindent
Lastly, varying the noise level $\sigma_n^2$ primarily affects the width of the credible interval, 
while the mean function remains largely unchanged. As $\sigma_n^2$ increases, the model accounts for higher observation noise, 
and uncertainty in the predictions increases accordingly. This relationship appears approximately linear in the visualizations.


\textcolor{red}{From previous section may be worth including}
\textbf{Credible Interval Behavior:} In Gaussian Process regression, the predictive variance at a new input $x^*$ is given by
\[
k(x^*, x^*) - k(x^*, X)(K_{XX}^{-1})k(X, x^*).
\]

With an RBF kernel, $k(x^*, X)$ will be near-zero if $x^*$ is farther than a few $\ell$'s away from all training inputs $X$. 
For small $\ell$, this situation happens frequently – any point outside a tiny neighborhood of training data effectively has $k(x^*, X) 
\approx 0$, so its predictive variance is roughly
\[
k(x^*, x^*) = \sigma_f^2
\]
(the prior variance). Thus, the 95\% credible interval reverts to roughly $\pm 2\sigma_f$ in large gaps or outside the data range.
For large $\ell$, on the other hand, $k(x^*, X)$ remains sizable over a much broader range, reducing the variance term. The uncertainty
only approaches the prior level far outside the data (several $\ell$'s away). In effect, a large $\ell$ flattens the covariance function
so that the GP “remembers” the training set far out, maintaining narrower error bars over a wider domain.



\subsubsection*{Adding Data: Prior to Posterior}
\label{sec: priortoposterior}

We have discussed how our prior distribution is dependent on the choice of kernel and the hyper-parameters of said kernel. In this section
we will discuss how we can update our prior distribution \ref{eq: Multivariate prior} to achieve our new prediction distribution distribution from which we can make
inferences. One of the key strengths of Gaussian Processes is that, given observations at training inputs \(X\) and setting kernel hyper-parameters \(\theta\)
we can make predictive inferences about the function value at any new test location \(x_*\).
By applying the standard conditional Gaussian formulas (see appendix \textcolor{red}{Must clean up this derivation in appendix} \ref{appendix:A} for the full derivation),
the posterior distribution of \(f(x_*)\) given \(\{X, f(X)\}\) is Gaussian and given by:


\begin{equation}
p\bigl(f(x_*) \mid f(X), X, X_*,\theta \bigr)
\;=\;
\mathcal{N}\!\Bigl(
m(x_*),\;\sigma^2(x_*)
\Bigr),
\label{eq: predictive_dist}
\end{equation}

\noindent
where

\begin{equation}
m(x_*) \;=\; \mu(x_*) \;+\;k(x_*, X)\,k(X, X)^{-1}\bigl[f(X) - \mu(X)\bigr],
\label{eq: predictive_mean}
\end{equation}

\begin{equation}
\sigma^2(x_*) \;=\;k(x_*, x_*) - k(x_*, X)\,k(X, X)^{-1}\,k(X, x_*).
\label{eq: predictive_variance}
\end{equation}

In these expressions:
\begin{itemize}
    \item \( \mu(\cdot) \) is the mean function ( We take this to be zero since we centre the data prior to prediction),
    \item \( k(X, X) \) is the covariance matrix among the observed training points,
    \item \( k(x_*, X) \) is the vector of cross-covariances between the test point \(x_*\) and the training inputs,
    \item \( k(x_*, x_*) \) is the prior variance at the test point itself.
\end{itemize}

\noindent
We now have an analytic function that can be evaluated to find the mean function value and variance at any input point.
This is a very useful property that Gaussian Processes possess. Figure~\ref{fig: priortoposterior} shows how we update our
distributions based on the training points. We initially plot samples taken from the prior distribution (Equation~\ref{eq: Multivariate prior}). 
After conditioning this distribution on one training point and obtaining the new predictive posterior distribution (Equation~\ref{eq: predictive_dist}), 
we see that the predictive mean passes exactly through the training point, has small variance around it, and then fans out farther away. 
Conditioning on two training points at opposite ends of our input domain creates an ellipse-shaped credible interval whose largest radius 
appears midway between the two training points. With more training points, the predictions align progressively closer to the true function 
and the variance becomes smaller.

\noindent
Note that this example uses a one-dimensional, noise-free function and a basic RBF kernel with fixed hyperparameters (\(\ell = 1\), \(\sigma^2 = 0.5\)). 
In practice the choice of Kernel function plays a pivotal role in the assumptions we make about the shape and general behaviour of our model. In the next section
we examine the different kernel choices available and the assumptions that each kernel encodes about our function structure, such as smoothness and periodicity.



\begin{figure}[H]
    \centering
        \includegraphics[width=\textwidth]{LatexPlots/1dplots/priortoposterior.png}
        \caption{Prior Distribution}
    \caption{1D Gaussian Process Regression: Prior to Posterior. This sequence shows how the GP prior transforms into a posterior as more data points are added. 
    The RBF kernel was used with hyper-parameters: $l = 1$, $\sigma^2 = 0.5$.The black line represents the true function. The blue is the mean of each posterio distribution. 
    The light blue shaded region is the credible interval and the grey lines are the samples drawn from each posterior/prior. }
    \label{fig: priortoposterior}
\end{figure}
\Sean{can add a potential link to animation here illustrating the nice distribution formed at each point}




\subsection{Handling Noise in our Data}
\label{sec: Handlingnoise}

So far, our discussion has assumed noise-free observations. However, real-world data is rarely clean, measurements often include some form of uncertainty. To make our Gaussian Process models more applicable to this real-world data,
we now explore how to incorporate noise into the GP framework.

\bigskip

\noindent
We assume that each observation includes the true function value plus Gaussian noise:
\begin{equation}
y_i = f(x_i) + \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0, \sigma_{n,i}^2),
\end{equation}
where \( \sigma_{n,i}^2 \) is the noise variance associated with input \( x_i \). This allows us to model both homoscedastic and heteroscedastic noise under a unified notation.

\bigskip

\noindent
Under this model, we have the following Gaussian assumptions:
\begin{align}
f &\sim \mathcal{N}(0, K), &&\text{(prior over the true function)} \\
y &\sim \mathcal{N}(0, K + \Sigma), &&\text{(distribution over noisy observations)},
\label{eq: prior_distribution_noise}
\end{align}
where \( \Sigma = \mathrm{diag}(\sigma_{n,1}^2, \sigma_{n,2}^2, \dots, \sigma_{n,n}^2) \) is the noise covariance matrix.

\bigskip
\noindent
This now updates our previous posterior mean and variance (eq: \ref{eq: predictive_mean} and \ref{eq: predictive_variance}) to a revised posterior:
\begin{equation}
    P(f_*|X,X_*,\theta,y) = \sim \mathcal{N}(m(f_*), Var(f_*))
    \label{eq: predictive_distribution_noise}
\end{equation}
\begin{equation}
m(f_*) = K_*^T (K_y)^{-1} y,
\label{eq: predictive_mean_noise}
\end{equation}
\begin{equation}
\text{Var}(f_*) = K_{**} - K_*^T (K_y)^{-1} K_*.
\label{eq: predictive_variance_noise}
\end{equation}
where \(K_y\) depends on how we handle our noise. There are three main cases of how we handle our noise

\subsubsection*{Homoscedastic Noise} 

In the homoscedastic case, we assume that all observations have the same noise level, meaning the noise variance is constant across the dataset:
\[
\sigma_{n,i}^2 = \sigma_n^2 \quad \forall i.
\]
This simplifies the noise covariance matrix \( \Sigma \) to a scalar multiple of the identity matrix:
\[
\Sigma = \sigma_n^2 I.
\]
The total covariance matrix of the observed data becomes:
\[
K(X, X) + \sigma_n^2 I = 
\begin{bmatrix}
k(x_1, x_1) + \sigma_n^2 & \cdots & k(x_1, x_n) \\
\vdots & \ddots & \vdots \\
k(x_n, x_1) & \cdots & k(x_n, x_n) + \sigma_n^2
\end{bmatrix}.
\]
Our prior distribution now becomes:
\begin{equation}
y \sim \mathcal{N}(0, K+\sigma_n^2 I)
\end{equation}
where $\sigma_n^2$ is a new parameter which effects the shape of the distribution. In figure \ref{fig: GPprior_hyperparams} we 
explored how the internal kernel hyper-parameters effect the shape of the sampes from our prior distribution. We now examin how the noise (i.e $\sigma_n^2$) effects samples from our prior distribution from \ref{eq:  predictive_distribution_noise}.
Our predictive distribution remains as in \ref{eq: predictive_distribution_noise}

\begin{figure}[H]
    \centering
        \includegraphics[width=\textwidth]{LatexPlots/1dplots/Kernel_noise.png}
        \caption{Sampling from the GP prior with mean 0 and covariance given by the RBF kernel.
        The plot shows the effect of the noise hyperparameter \(\sigma_n^2\) on the GP prior.We fix the signal variance to 1 and length scale to 0.5.}
    \label{fig: kernel_noise}
\end{figure}
\noindent
Our posterior distribution is as in eqn \ref{eq:  predictive_distribution_noise} with \(K_y = K(X, X) + \sigma_n^2 I \) for some constant \(\sigma_n\)


\subsubsection*{Heteroscedastic Noise}  
\textbf{Known Noise}
\noindent
In this case, the noise variance changes across the input space—some observations are noisier than others. If we know the individual noise variances \( \sigma_i^2 \) for each training input \( x_i \), we incorporate them by adding a diagonal noise matrix to the kernel:
\[
K(X, X) + \Sigma = 
\begin{bmatrix}
k(x_1, x_1) + \sigma_1^2 & \cdots & k(x_1, x_n) \\
\vdots & \ddots & \vdots \\
k(x_n, x_1) & \cdots & k(x_n, x_n) + \sigma_n^2
\end{bmatrix},
\]
where \( \Sigma = \text{diag}(\sigma_1^2, \sigma_2^2, \dots, \sigma_n^2) \).
In this case the noise is not found as a hyper-parameter but instead just added to the diagonal of the covariance matrix.
Our posterior distribution is as in eqn \ref{eq:  predictive_distribution_noise} with \(K_y =K(X, X) + \Sigma \) where \(\Sigma\) is the known noise of our data. 

\vspace{1em}
\noindent
\textbf{Learning Noise over the Input Space}
\noindent
If the noise variance is unknown but varies across the input space, we can model it as a function. 
This is done by building a kernel that captures both smooth, global trends and rough, local fluctuations. 
In practice, this means building an additive kernel made up of sub-kernels.For example, as seen in Table~\ref{tab:kernel-examples}, some kernels like the Matern, Laplacian, or Rational Quadratic capture local variations well (interpreted as noise),
while others like the RBF capture broader, smoother structure. By combining these, we can allow one kernel component to model the general structure of the function, and the other to model the heteroscedastic noise behavior. 
Our posterior distribution is as in eqn \ref{eq:  predictive_distribution_noise} with :

\begin{equation}
    K_y = \theta_1 K_1(X, X) + \theta_2 K_2(X, X),
    \label{eq:additive_kernel}
\end{equation}
where \( K_1 \) and \( K_2 \) are distinct kernels chosen to capture different aspects of the data. 
The coefficients \( \theta_1 \) and \( \theta_2 \) are parameters that control the relative contribution of each kernel component.

\subsubsection*{Monte Carlo Sampling of Noise}  

This technique can be applied to both homoscedastic and heteroscedastic noise settings. Instead of modifying the kernel matrix analytically, we simulate the effect of noise by generating multiple noisy realizations of the observed outputs.
\noindent
We assume the observation noise is Gaussian:
\[
\epsilon_i \sim \mathcal{N}(0, \sigma_i^2),
\]
and therefore, the observed data \( y \) is a noisy version of the true latent function values:
\[
y = f + \epsilon, \quad \text{with } y \sim \mathcal{N}(f, \Sigma),
\]
where \( \Sigma = \mathrm{diag}(\sigma_1^2, \dots, \sigma_n^2) \).
To simulate this, we generate \( M \) noisy samples of the observations:
\begin{equation}
    y^{(s)} = y + \epsilon^{(s)}, \quad \epsilon^{(s)} \sim \mathcal{N}(0, \Sigma)
\end{equation}
For each sampled dataset \( y^{(s)} \), we compute an independent GP posterior:
\begin{equation}
    p(f_* \mid X, X_*, \theta, y^{(s)}).
\end{equation}
To obtain the final predictive distribution, we marginalize over these sampled posteriors:
\begin{equation}
    p(f_* \mid X, X_*, \theta, y) = \int p(f_* \mid X, X_*, \theta, y^{(s)}) \, p(y^{(s)} \mid y) \, dy^{(s)}.
\end{equation}
This integral is intractable in closed form, so we approximate it using Monte Carlo integration:
\begin{equation}
    p(f_* \mid X, X_*, \theta, y) \approx \frac{1}{M} \sum_{s=1}^{M} p(f_* \mid X, X_*, \theta, y^{(s)}).
\end{equation}


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{LatexPlots/1dplots/GPR_noise_comparison.png}
    \caption{Samples drawn from a zero-mean Gaussian Process prior with varying noise assumptions.
    \textbf{Left:} Homoscedastic noise, where a constant noise variance \(\sigma_n^2 = 0.5\) is added uniformly across all inputs.
    \textbf{Middle:} Heteroscedastic noise, where individual noise variances \(\sigma_i^2\) are known and drawn from \(\mathcal{N}(0, 0.5)\), resulting in a diagonal noise covariance.
    \textbf{Right:} Monte Carlo sampling of noisy observations, where multiple noisy realizations are generated from \(\mathcal{N}(f(x), \epsilon^2)\)}
    \label{fig:noise_comparison}
\end{figure}

\Sean{Own words}

\noindent
We can observe in Figure~\ref{fig:noise_comparison} how different noise assumptions influence samples drawn from the Gaussian Process prior. 
In the homoscedastic case (left), the output exhibits consistent fluctuations across the entire domain due to a constant noise variance applied uniformly to all inputs. 
In contrast, the heteroscedastic case (middle) introduces input-dependent noise, resulting in regions of smoothness interspersed with abrupt variations—reflecting the fact that each output has its own associated noise level.
Finally, in the Monte Carlo noise sampling approach (right), we generate multiple realizations by perturbing the data with different noise samples. This highlights the diversity of plausible functions consistent with the observed data and captures the full range of uncertainty introduced by noisy observations.
 



\subsection{Hyper-parameters}
\label{sec: Hyper_parameters}

\noindent
Until now, all predictive distributions such as Equations~\ref{eq: predictive_distribution_noise} and~\ref{eq: predictive_dist} have been conditioned on fixed kernel hyperparameters. As demonstrated in Figures~\ref{fig: GPprior_hyperparams} and~\ref{fig: kernel_noise}, these hyperparameters have a significant influence on the structure and behaviour of the Gaussian Process, shaping both the prior and posterior distributions. 

\noindent
In practice, these hyperparameters are not known a priori and must be inferred from the data. To do so, we seek the set of hyperparameters that best explain the observed data by maximising the \textit{log marginal likelihood}, a method detailed in Chapter 5 of \cite{bible}.

\bigskip
\noindent
From Equation~\ref{eq:  predictive_distribution_noise}, we have:
\[
y \sim \mathcal{N}(0, K + \sigma_n^2 I)
\]
where \( K \) is the kernel matrix computed from the training inputs \( X \), and \( \sigma_n^2 \) is the noise variance.

\noindent
This implies that the marginal likelihood (i.e., the probability of the observed outputs \( y \) given the inputs \( X \) and hyperparameters \( \theta \)) is given by the multivariate Gaussian density:
\[
p(y \mid X, \theta) = \frac{1}{(2\pi)^{n/2} |K_y|^{1/2}} \exp\left( -\frac{1}{2} y^\top K_y^{-1} y \right)
\]
where \( K_y = K + \sigma_n^2 I \).

\bigskip

\noindent
Taking the logarithm of this expression yields the \textit{log marginal likelihood}:
\begin{equation}\label{eq: 5}
\log p(y \mid X, \theta) = -\frac{1}{2} y^\top K_y^{-1} y - \frac{1}{2} \log |K_y| - \frac{n}{2} \log 2\pi
\end{equation}

\noindent
Our goal is to maximise this log marginal likelihood with respect to the hyperparameters \( \theta \), which typically includes the kernel lengthscale, signal variance, and noise variance. Once optimal values are found, we can use them to make accurate posterior predictions.



\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{LatexPlots/1dplots/Variancevslength.png}
    \end{subfigure}
    \hspace{0.05\textwidth}
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{LatexPlots/1dplots/Variancevsnoise.png}
    \end{subfigure}
    \vspace{0.05em}
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{LatexPlots/1dplots/lengthvsnoise.png}
    \end{subfigure}
    \hspace{0.05\textwidth}
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{LatexPlots/1dplots/LogLike3params.png}
    \end{subfigure}
    \caption{For a GPR with noise we are forced to optimise a length hyper-parameter, a variance hyper-parameter and a noise hyper-parameter. 
    Here we have kept one parameter constant on each graph and compared the log likelihood space varying the other two parameters. 
    In graph one the noise is set at 0.1, in graph 2 the length is set at 0.5, in graph 3 the variance is set at 1.5. 
    In the final graph we plot a 3-dimensional scatter plot and illustrate the point estimate given by the optimisation algorithm as the black dot.This point is located at
    $(\sigma^2 = 1.16, l = 0.109$ noise$ = 0.105)$}
   \label{figure: Optimising Hyper-params}
\end{figure} 


\subsubsection{Cross Validation}

While maximising our parameters with respect to the log likelihood is common practice [ch 5, \cite{bible}] details how we may overfit our model to the training data.
To account for this we use cross validation where we split our data into disjoint sets. One set is used to train our model (find the hyper-parameters) and the other set is used to monitor performance.
If we split our data into K sets every iteration we train on K-1 sets, and validate on the kth set. We iterate this until all sets have been used to validate our model. 

\Sean{Have to think about this model selection is what I am talking about here. Maybe leave out cross val here because I am not doing cross val for hyper-parameters}


\section{Quantifying Uncertainty and Evaluating Model Accuracy}
\label{sec:uncertainty_and_evaluation}
\subsection{Monte Carlo Markov Chain (MCMC): A Posterior over Hyperparameters}
\label{sec: MCMC}

\subsubsection*{Why MCMC and the Mathematics Behind It}

We have discussed the role of kernels and how their hyperparameters affect the posterior distribution. From Figures~\ref{fig: GPprior_hyperparams} and~\ref{figure: Optimising Hyper-params}, we observed that when optimising hyperparameters, there is not necessarily a single "best" solution, but rather a region of valid values that explain the data well.

\noindent
Up to this point, the models we considered have relied on point estimates—selecting the hyperparameters that maximise the log marginal likelihood and using them for predictions. However, this approach ignores the uncertainty between hyperparameters that yield similarly high log-likelihood scores.
To address this uncertainty, we adopt a Bayesian perspective by using Markov Chain Monte Carlo (MCMC) methods to sample from the posterior distribution over hyperparameters. This approach allows us to move beyond point estimates and instead capture a full distribution that reflects uncertainty and variability in the hyperparameters.
By leveraging this posterior distribution, we gain a more comprehensive understanding of the model's behaviour. It allows for more robust predictions, better uncertainty quantification, and improved decision-making—especially in cases where multiple hyperparameter settings are plausible.

\bigskip
\noindent
When getting a point-estimate we maximised the log likelihood from equation \ref{eq: 5}. Now we want to build a distribution over the hyper-parameters. We have that:
\[
p(\theta \mid \mathbf{y}, X) = \frac{p(\mathbf{y} \mid X, \theta) \, p(\theta)}{p(\mathbf{y} \mid X)}
\]
where:
\begin{itemize}
    \item \( p(\mathbf{y} \mid X, \theta) \) is the likelihood,
    \item \( p(\theta) \) is the prior over hyperparameters,
    \item \( p(\mathbf{y} \mid X) \) is the marginal likelihood, acting as a normalising constant.
\end{itemize}

\noindent
Since \( p(\mathbf{y} \mid X) \) is often intractable, we sample from the unnormalised posterior using MCMC:
\[
p(\theta \mid \mathbf{y}, X) \propto p(\mathbf{y} \mid X, \theta) \, p(\theta)
\]

\noindent
Using MCMC, we generate samples \( \{\theta^{(s)}\}_{s=1}^S \sim p(\theta \mid \mathbf{y}, X) \) from this posterior.


\subsubsection*{Implementing MCMC}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{LatexPlots/MCMC_Flow.png}
    \caption{Overview of the MCMC sampling procedure for Gaussian Process hyperparameter inference.
     This pipeline samples from the posterior \( p(\theta \mid \mathbf{y}, X) \) using a Metropolis-Hastings Gaussian proposal and an ensemble of walkers.}
    \label{fig:MCMC flowchart}
\end{figure}

\noindent
Before starting MCMC, we must decide which model we want to build the hyperparameter posterior for. This involves selecting one of the six kernels outlined in Section~\ref{sec: Kernels}, along with one of the three noise-modelling approaches described in Section~\ref{sec: Handlingnoise}. Once this model structure is fixed, we obtain initial point estimates for the hyperparameters by maximising the log marginal likelihood, as discussed in Section~\ref{sec: Hyper_parameters}.

\noindent
We then construct a multivariate normal distribution centred at this point estimate and sample from it to initialise each walker. From there, the walkers explore the hyperparameter space using a Gaussian proposal distribution with a specified covariance. At each step, we compute the sum of the log likelihood and log prior. The proposed sample is then accepted or rejected using the Metropolis-Hastings criterion \textcolor{red}{Could give more detail here}. This process is repeated for a fixed number of steps to generate a large set of samples.

\noindent
To ensure convergence and sample independence, we discard the first 100 samples from each walker as burn-in and apply a thinning factor of 15—retaining every 15\textsuperscript{th} sample. The resulting collection of samples forms our posterior distribution over hyperparameters, which we visualise using a kernel density estimate (KDE).


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{LatexPlots/1dplots/MCMCwalkers.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{LatexPlots/1dplots/MCMCdistribution.png}
    \end{subfigure}
    \caption{This MCMC is run using a gpr with a RBF Kernel with White Kernel for noise.Left: A plot of the space sampled by each walker in my MCMC run (burnin=100 and thin=15). \quad Right: A plot of the distribution of the hyper-parameters built using a Gaussian KDE from the MCMC samples.}
    \label{fig:MCMCresults}
\end{figure}
\noindent
We can see from the distributions in \ref{fig:MCMCresults} that the variance hyper-paramterer is almost bi-modal (i.e has two significant peaks). For our bi-model hyper-parameter we can see that the mean parameter, peak parameter and point estimate parameter differ significantly.
In this scenario any singular point estimate will lose a lot of information about the distribution particularly for the variance hyper-parameter. To resolve this we can instead of picking a singular point build
the hyper-parameter uncertainty into our final predictive distribution.


\noindent
We previously had our predictive distribution as :
\[
p(f_* \mid \mathbf{y}, X, X_*, \theta),
\]
where predictions were made conditional on a fixed set of hyperparameters \( \theta \). Now, we marginalise over the posterior distribution of \( \theta \) to account for hyperparameter uncertainty:
\[
p(f_* \mid \mathbf{y}, X, X_*) = \int p(f_* \mid \mathbf{y}, X, X_*, \theta) \, p(\theta \mid \mathbf{y}, X) \, d\theta.
\]

\noindent
Since this integral is analytically intractable, we approximate it using the MCMC samples \( \{\theta^{(s)}\}_{s=1}^S \):
\[
p(f_* \mid \mathbf{y}, X, X_*) \approx \frac{1}{S} \sum_{s=1}^{S} p(f_* \mid \mathbf{y}, X, X_*, \theta^{(s)}).
\]

\noindent
This entails constructing a full posterior predictive distribution for each set of sampled hyperparameters and then averaging across all predictions. In practice, we compute the final predictive mean and variance using the law of total variance:
\[
\mathbf{E}[f_*] \approx \frac{1}{S} \sum_{s=1}^{S} \mu^{(s)}(f_*),
\]
\[
\text{Var}[f_*] \approx \frac{1}{S} \sum_{s=1}^{S} \left[ \sigma^{2(s)}(f_*) + \left(\mu^{(s)}(f_*)\right)^2 \right] - \left( \mathbf{E}[f_*] \right)^2,
\]
where \( \mu^{(s)}(f_*) \) and \( \sigma^{2(s)}(f_*) \) are the predictive mean and variance obtained from the \( s \)-th sampled hyperparameter configuration \( \theta^{(s)} \).


\noindent
This procedure yields a predictive distribution that fully reflects both the uncertainty in the data and the uncertainty in the model's hyperparameters



\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{LatexPlots/1dplots/MCMCmeangpr.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{LatexPlots/1dplots/MCMCpeakgpr.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{LatexPlots/1dplots/MCMCaveragegpr.png}
    \end{subfigure}
    \caption{
        Left: This is the GPR plotted with the mean hyper-parameters
        Middle: This is the GPR plotted with the peak hyper-parameters
        Right: This is the full predictive distribution over all the hyper-parameter samples
     }
\end{figure}



\subsection{Model Evaluation Metrics}
\label{sec:metrics}

In figure \ref{tab:metrics-comparison}, we made a graphical representation of four out of six metrics used to evaluate our model’s accuracy. \cite{bible} discusses how these metrics provide a balanced assessment of model performance.
The Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) measure the average deviation of predictions from the true values, with RMSE penalizing larger errors more heavily.
The coefficient of determination \( R^2 \) quantifies how well the model predicts relative to the mean of the test set. It is computed as \( 1 \) minus the ratio of the squared residuals to the total variance. A value closer to \( 1 \) indicates better predictive performance.
The adjusted \( R^2 \) (\(\bar{R}^2\)) accounts for model complexity by penalizing excessive predictor variables, preventing overfitting.
The Figure of Merit (FOM) evaluates the ratio of a point’s prediction error to its associated standard deviation. A FOM near \( 1 \) is ideal, indicating that the model’s uncertainty estimates are well-calibrated. A FOM \( \ll 1 \) suggests an overly conservative model with large uncertainty, while a FOM \( \gg 1 \) may indicate overconfidence, failing to capture true variability.
The Pearson correlation coefficient measures the linear relationship between predictions and true values. A correlation of \( 1 \) (\(-1\)) signifies a perfect positive (negative) linear relationship, whereas a correlation of \( 0 \) indicates no linear association.



\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{4} % Adjust row spacing
    \setlength{\tabcolsep}{2pt} % Adjust column spacing
    \small % Reduce text size

    \begin{tabular}{|>{\centering\arraybackslash}m{2.5cm}|*{4}{>{\centering\arraybackslash}m{3cm}|}} 
        \hline
        \textbf{Metric Name} & \textbf{RMSE} & \textbf{\(R^2\)} & \textbf{FOM} & \textbf{Pearson Coefficient} \\ 
        \hline
        \textbf{Formula} & 
        \( \sqrt{\frac{1}{N} \sum (y_i - \hat{y}_i)^2} \)   & 
        \( 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2} \) &    
        \( \frac{RMSE}{\sigma} \) &  
        \( \frac{\text{cov}(y - \hat{y})}{\sigma_y \sigma_{\hat{y}}} \) \\ 
        \hline
        \textbf{Visual Illustration} &  
        \adjustbox{valign=c}{\includegraphics[width=3cm]{LatexPlots/1dplots/MAE.png}} &  
        \adjustbox{valign=c}{\includegraphics[width=3cm]{LatexPlots/1dplots/r2.png}} &  
        \adjustbox{valign=c}{\includegraphics[width=3cm]{LatexPlots/1dplots/fom.png}} &  
        \adjustbox{valign=c}{\includegraphics[width=3cm]{LatexPlots/1dplots/pearson.png}} \\  
        \hline
    \end{tabular}
    
    \caption{Comparison of different performance metrics used in evaluating models.  
    RMSE, \( R^2 \), FOM, and the Pearson Coefficient are included.  
    MAE is similar to RMSE but without squaring errors.  
    Adjusted \( R^2 \) accounts for the number of predictors and is slightly modified from \( R^2 \).
    The actual metrics for each graph are: RMSE = 0.2, \( R^2 \) = 0.6, FOM = 1.09, Pearson correlation = 0.8.}
    
    \label{tab:metrics-comparison}
\end{table}

\section{Multi-Dimensional Gaussian Process Regression}

\begin{itemize}
    \item Here we want to bring up how we can have a different length parameter for each dimension of the data.
    \item Get a few plots to demonstrate this maybe.
    \item Important to note noise modelling for all dimensions is the same since just adding to the diagonal of the covariance matrix.
    \item Mention how hyper-parameter optimisatino changes in the larger parameter space
\end{itemize}


\section{Method}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{LatexPlots/Flowchart.png}
    \caption{My Process Flowchart}
    \label{fig:flowchart}
\end{figure}

\subsection{Data Description}
- Dimensionality choices (4D vs 7D)
- Summary of input features
- How the data was generated or collected (e.g., from simulations or experiments)

\subsubsection*{Kernel Choices}
I built my selection of models using 5 kernels discussed in section \ref{sec: Kernels} these kernels are the 
Radial Basis Function (RBF), the Rational Quadratic Function (RQF), the Matern Kenrel (Mat), the periodic kernel (Expsine) and the laplace kernel (lapl).

\subsubsection*{Noise approach}
I used each noise type discussed in section \ref{sec: Handlingnoise}, this includes treating noise as a hyper-parameter (homoscedastic noise), 
using our known noise (heterscedastic), using combined kernels to model the noise (heterscedastic), using monte carlo sampling of the noise (heterscedastic). 

\subsubsection*{Hyper parameters}
For all my models I maximised the marginal log-likelihood for parameter selection as discussed in \ref{sec: Hyper_parameters}. Then after finalising models
I also ran Monte Carlo Markov Chain simulations to build a posterior over the hyper-parameters and understnad the uncertainty/ distribution associated with each hyper-parameter.

\subsubsection*{Metrics Calculation}
In order to compare these models effectively I evaluated each model using the metrics discussed in section \ref{sec:metrics} and then compared the metrics.

\subsubsection*{Cross Valiation}
To ensure model's generalised well and weren't biased to a certain training set I used 10 fold cross validation and on each fold calculated the metrics for comparison.


\section{Results}

\subsection{Cross-Validation Performance}
- Plots of metrics per model type
- Ranking tables or summaries

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{LatexPlots/CV_plots/metrics_compared.png}
    \caption{Comparing how simlar results the metrics give in a dendrogram}
    \label{fig:Dendrogram}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{LatexPlots/CV_plots/clustermetrics.png}
        \caption{Results from cross-validation.}
        \label{fig:CVoverallmodels}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{LatexPlots/CV_plots/r2vsfom.png}
        \caption{Comparing clustered model performance by $R^2$ and FoM.}
        \label{fig:r2vsfom}
    \end{subfigure}
    \caption{Cross-validation metric trends and a performance trade-off visualization.}
    \label{fig:CV_sidebyside}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{LatexPlots/CV_plots/boxplots.png}
    \caption{Seeing how the best models performed over different clusters}
    \label{fig:boxplots}
\end{figure}


\subsection{Training on 90\% of Data}
- Final model evaluations on held-out 10%
- Error bars, confidence intervals, diagnostics

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{LatexPlots/final_gps_plots/metrics_of_finalists.png}
    \caption{Metrics of best 8}
    \label{fig:best8_metrics}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{LatexPlots/final_gps_plots/metric_of_finalists_comparison.png}
    \caption{Comparing metrics}
    \label{fig:comparing_metrics}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{LatexPlots/final_gps_plots/final_gps_ycuts.png}
    \caption{All 8 gps with cutting their y-axis}
    \label{fig:best8_ycuts}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{LatexPlots/final_gps_plots/final_gps_xcuts.png}
    \caption{All 8 gps with cutting their x-axis}
    \label{fig:best8_xcuts}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{LatexPlots/final_gps_plots/gps_rbfmatern_xcuts.png}
    \caption{Examining my chosen model RBF Matern kernel}
    \label{fig:RBF_Matern_xcuts}
\end{figure}






\subsection{Top Model Selection and Analysis}
- Your top 3 models
- Summary of their performance
- MCMC trace plots or hyperparameter uncertainty discussion



\appendix

\section{Appendix A: Derivation of Predictive Distribution}
\label{appendix:A}
Using Bayes' Theorem applied to continuous probabilities, we have:
\[
p(f_* | f) = \frac{p(f_*, f)}{p(f)}.
\]
we have:
$$p(f_*,f) = \frac{1}{2\pi\sqrt{\mathbf{|C|}}}\exp \left(-\frac{1}{2} 
\begin{bmatrix} f \\ f_*  \end{bmatrix}^T\mathbf{C^{-1}}\begin{bmatrix} f  \\ f_* \end{bmatrix}\right)$$
and 
\[
p(f) = \frac{1}{\sqrt{2\pi} |K|^{1/2}}
\exp \left(-\frac{1}{2} f^T K^{-1} f \right).
\]

\noindent
\textbf{Legend}
\begin{itemize}
    \item \textbf{Covariance matrices:}
    \begin{itemize}
        \item \( K = K(X, X) \): Covariance matrix of the training inputs.
        \item \( K_{**} = K(X_*, X_*) \): Covariance matrix of the test inputs.
        \item \( K_* = K(X, X_*) = K(X_*, X)^\top \): Cross-covariance between training and test inputs.
    \end{itemize}
    
    \item \textbf{Joint covariance matrix:}
    \[
    C = \begin{bmatrix}
    K & K_* \\
    K_*^\top & K_{**}
    \end{bmatrix}
    \]
    
    \item \textbf{Determinant of \( \mathbf{C} \):}
    \[
    |\mathbf{C}| = K K_{**} - K_* K_*^\top
    \]
    
    \item \textbf{Inverse of \( \mathbf{C} \):}
    \[
    \mathbf{C}^{-1} = \frac{1}{|\mathbf{C}|}
    \begin{bmatrix}
    K_{**} & -K_* \\
    -K_*^\top & K
    \end{bmatrix}
    \]
    
    \item \textbf{Mean functions:}
    \[
    m(X) = m(X_*) = 0
    \]
\end{itemize}



\section{Appendix B: Kernel Formulas}
\label{appendix:B}
\subsubsection*{Radial Basis Function (RBF) Kernel}
\[
k(x, x') = \sigma_f^2 \exp\left( -\frac{(x - x')^2}{2\ell^2} \right)
\]
This kernel assumes smooth and infinitely differentiable functions, modeling local variations.


\subsubsection*{Rational Quadratic Kernel}
\[
k(x, x') = \sigma_f^2 \left( 1 + \frac{(x - x')^2}{2 \alpha \ell^2} \right)^{-\alpha}
\]
This kernel can be seen as a scale mixture of RBF kernels, allowing for multi-scale behavior.


\subsubsection*{Periodic Kernel}
\[
k(x, x') = \sigma_f^2 \exp\left( -\frac{2}{\ell^2} \sin^2\left( \frac{\pi (x - x')}{p} \right) \right)
\]
This kernel models repeating structures with period \( p \).

\subsubsection*{Matern Kernel}
\[
k(x, x') = \sigma_f^2 \frac{2^{1-\nu}}{\Gamma(\nu)} \left( \frac{\sqrt{2\nu} |x - x'|}{\ell} \right)^\nu K_\nu\left( \frac{\sqrt{2\nu} |x - x'|}{\ell} \right)
\]
The Matern kernel allows for controlling the smoothness of functions via the parameter \( \nu \).


\subsubsection*{Laplace (Exponential) Kernel}
\[
k(x, x') = \sigma_f^2 \exp\left( -\gamma |x - x'| \right)
\]
Equivalent to the Matern kernel with \( \nu = \frac{1}{2} \), this kernel models rougher functions.


\subsubsection*{Linear (Dot-Product) Kernel}
\[
k(x, x') = \sigma_b^2 + x^\top x'
\]
This kernel grows with the similarity (inner product) between inputs, and it allows the function to vary globally. Since it depends directly on the values of \( x \) and \( x' \), not just their difference, it is non-stationary. It is particularly useful for modeling linear trends.


\noindent
We obtain the posterior predictive distribution:
\begin{equation}\label{eq: 1}
p(f_* | f) = \frac{1}{(2\pi)^{n/2} |K_{**} - K_*^T K^{-1} K_*|^{1/2}} 
\exp \left( 
-\frac{1}{2} 
(f_* - K_*^T K^{-1} f)^T 
(K_{**} - K_*^T K^{-1} K_*)^{-1} 
(f_* - K_*^T K^{-1} f) 
\right).
\end{equation}



% %\printbibliography
% \bibliographystyle{acm}
% \bibliography{references}
\bibliography{references} 

\end{document}


