\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{acm}
\providecommand \oddpage@label [2]{}
\citation{Ogpaper}
\citation{GRbook}
\citation{GRbook}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Intro}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Gravitational Waves Background}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Plane Wave Solution from General Relativity}{1}{subsection.1.1.1}\protected@file@percent }
\newlabel{sec:GRintro}{{1.1.1}{1}{Plane Wave Solution from General Relativity}{subsection.1.1.1}{}}
\citation{intoGRSarp}
\@writefile{toc}{\contentsline {subsubsection}{Leading-Order Power Emission by Gravitational Waves}{2}{section*.5}\protected@file@percent }
\newlabel{subsec:GW_power}{{1.1.1}{2}{Leading-Order Power Emission by Gravitational Waves}{section*.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Quasi-circular Inspiral of two Point Masses}{2}{section*.6}\protected@file@percent }
\newlabel{subsec:two_mass_example}{{1.1.1}{2}{Quasi-circular Inspiral of two Point Masses}{section*.6}{}}
\newlabel{eq:E_dot_2}{{1.9}{2}{Quasi-circular Inspiral of two Point Masses}{equation.1.9}{}}
\newlabel{eq:Energy}{{1.10}{2}{Quasi-circular Inspiral of two Point Masses}{equation.1.10}{}}
\citation{GRbook}
\citation{GRbook}
\citation{mismatch,Ogpaper}
\newlabel{eq:GW_strain}{{1.11}{3}{Quasi-circular Inspiral of two Point Masses}{equation.1.11}{}}
\newlabel{eq:hp_time}{{1.12a}{3}{Quasi-circular Inspiral of two Point Masses}{equation.1.12a}{}}
\newlabel{eq:hc_time}{{1.12b}{3}{Quasi-circular Inspiral of two Point Masses}{equation.1.12b}{}}
\newlabel{eq:char_strain}{{1.12c}{3}{Quasi-circular Inspiral of two Point Masses}{equation.1.12c}{}}
\newlabel{eq:hfreq}{{1.13}{3}{Quasi-circular Inspiral of two Point Masses}{equation.1.13}{}}
\newlabel{eq:hplus_freq}{{1.13a}{3}{Quasi-circular Inspiral of two Point Masses}{equation.1.13a}{}}
\newlabel{eq:hcross_freq}{{1.13b}{3}{Quasi-circular Inspiral of two Point Masses}{equation.1.13b}{}}
\newlabel{eq:A_amp}{{1.14a}{3}{Quasi-circular Inspiral of two Point Masses}{equation.1.14a}{}}
\newlabel{eq:psi_cross}{{1.14b}{3}{Quasi-circular Inspiral of two Point Masses}{equation.1.14b}{}}
\newlabel{eq:psi_plus}{{1.14c}{3}{Quasi-circular Inspiral of two Point Masses}{equation.1.14c}{}}
\@writefile{toc}{\contentsline {subsubsection}{Introducing the Waveform Mismatch}{3}{section*.7}\protected@file@percent }
\newlabel{subsec:mismatch_intro}{{1.1.1}{3}{Introducing the Waveform Mismatch}{section*.7}{}}
\citation{linearcombination}
\citation{Bayesianapproach}
\citation{Ogpaper}
\newlabel{eq:mismatch_def}{{1.15}{4}{Introducing the Waveform Mismatch}{equation.1.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}GW Approximation using Bayesian Methods}{4}{subsection.1.1.2}\protected@file@percent }
\newlabel{sec:GW_and_Bayes}{{1.1.2}{4}{GW Approximation using Bayesian Methods}{subsection.1.1.2}{}}
\citation{Ogpaper}
\citation{NRsimulation}
\citation{bestNRfitS}
\citation{NRfitMP}
\citation{NRfitMT}
\citation{Ogpaper}
\citation{Ogpaper}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Motivating NR Method Figure}}{5}{figure.caption.8}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:projectmotivation}{{1.1}{5}{Motivating NR Method Figure}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Project Motivation}{5}{subsection.1.1.3}\protected@file@percent }
\citation{bestNRfitS}
\citation{NRsurrogate}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Data Description}{6}{subsection.1.1.4}\protected@file@percent }
\newlabel{sec:data_description}{{1.1.4}{6}{Data Description}{subsection.1.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Visualising a binary black hole system. The two black holes with masses \(M_1\) and \(M_2\) orbit each other in a quasi-circular orbit, each with spin vectors \(\mathbf  {S}_1\) and \(\mathbf  {S}_2\). The total spin is \(\mathbf  {S}_{\text  {total}} = \mathbf  {S}_1+\mathbf  {S}_2\) and \(\mathbf  {L}\) is the orbital angular momentum. The spin projections \( \chi _{\parallel } \) and \( \chi _{\perp } \) are the components of the total spin that are parallel and perpendicular to \( \mathbf  {L} \), respectively.}}{6}{figure.caption.9}\protected@file@percent }
\newlabel{fig:gr_diagram}{{1.2}{6}{Visualising a binary black hole system. The two black holes with masses \(M_1\) and \(M_2\) orbit each other in a quasi-circular orbit, each with spin vectors \(\mathbf {S}_1\) and \(\mathbf {S}_2\). The total spin is \(\mathbf {S}_{\text {total}} = \mathbf {S}_1+\mathbf {S}_2\) and \(\mathbf {L}\) is the orbital angular momentum. The spin projections \( \chi _{\parallel } \) and \( \chi _{\perp } \) are the components of the total spin that are parallel and perpendicular to \( \mathbf {L} \), respectively}{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces  Visualisation of the input data across reduced dimensions. Left: A 3D scatter plot of all data points for fixed total mass \(w = 0.25\) ($M_\text  {tot}=37.5 M_\odot $). Centre: A 2D slice of the data at rescaled symmetric mass ratio \(z = 0\) ($q=0.404$), interpolated over spin components. Right: A 1D cut through the data at \(x = 0.8\) showing variation across \(y\). When using raw data, interpolation is needed between samples, but GPR provides an analytic model that can be directly evaluated without interpolation. }}{7}{figure.caption.10}\protected@file@percent }
\newlabel{fig:visualising_data}{{1.3}{7}{Visualisation of the input data across reduced dimensions. Left: A 3D scatter plot of all data points for fixed total mass \(w = 0.25\) ($M_\text {tot}=37.5 M_\odot $). Centre: A 2D slice of the data at rescaled symmetric mass ratio \(z = 0\) ($q=0.404$), interpolated over spin components. Right: A 1D cut through the data at \(x = 0.8\) showing variation across \(y\). When using raw data, interpolation is needed between samples, but GPR provides an analytic model that can be directly evaluated without interpolation}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Theory}{8}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Gaussian Process Regression \textbf  {GPR} Background}{8}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Introduction and Roadmap}{8}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Gaussian Proces Regression Background}{8}{subsection.2.1.2}\protected@file@percent }
\newlabel{sec: GP_backgroound}{{2.1.2}{8}{Gaussian Proces Regression Background}{subsection.2.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Definition of a Gaussian Process}{8}{section*.11}\protected@file@percent }
\newlabel{sec: Definition_of_GP}{{2.1.2}{8}{Definition of a Gaussian Process}{section*.11}{}}
\newlabel{eq: Initial_GP_distribution}{{2.1}{8}{Definition of a Gaussian Process}{equation.2.1}{}}
\newlabel{eq: meandef}{{2.2}{8}{Definition of a Gaussian Process}{equation.2.2}{}}
\citation{bible}
\newlabel{eq: kerneldef}{{2.3}{9}{Definition of a Gaussian Process}{equation.2.3}{}}
\newlabel{eq: Multivariate_distribution}{{2.6}{9}{Definition of a Gaussian Process}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{The Prior Distribution}{9}{section*.12}\protected@file@percent }
\newlabel{sec: prior_dist}{{2.1.2}{9}{The Prior Distribution}{section*.12}{}}
\newlabel{eq: Multivariate prior}{{2.7}{9}{The Prior Distribution}{equation.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Kernel Functions}{9}{subsection.2.1.3}\protected@file@percent }
\newlabel{sec: Kernels}{{2.1.3}{9}{Kernel Functions}{subsection.2.1.3}{}}
\citation{bible}
\citation{kernelcookbook}
\citation{bible}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Sampling from the GP prior with zero mean and an RBF kernel ($\ell = 0.5$, $\sigma _f^2 = 1$). The first plot shows three sample functions drawn from the prior distribution. The second plot visualizes the covariance matrix as a heatmap, revealing the strength of correlations between inputs. The final three subplots display joint distributions between selected input pairs. These distributions are multivariate distributions with mean zero and covariance matrix defined by a \(2 \times 2\) where the diagonals are 1 and the off diagonals are the correlation between the selected input points. The contours are drawn at multiples of \(0.5 \sigma \) indicating confidence regions for each distribution}}{10}{figure.caption.13}\protected@file@percent }
\newlabel{fig: samples_from_GP_prior}{{2.1}{10}{Sampling from the GP prior with zero mean and an RBF kernel ($\ell = 0.5$, $\sigma _f^2 = 1$). The first plot shows three sample functions drawn from the prior distribution. The second plot visualizes the covariance matrix as a heatmap, revealing the strength of correlations between inputs. The final three subplots display joint distributions between selected input pairs. These distributions are multivariate distributions with mean zero and covariance matrix defined by a \(2 \times 2\) where the diagonals are 1 and the off diagonals are the correlation between the selected input points. The contours are drawn at multiples of \(0.5 \sigma \) indicating confidence regions for each distribution}{figure.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces  Visual comparison of common kernel functions and their effect on Gaussian process priors. Each column shows the kernel shape $k(x, x')$, samples from the corresponding GP prior, and a summary of the structure it imposes. All kernels were evaluated using a lengthscale parameter $\ell = 1$ (except where noted). For the Matern kernel, $\nu = 0.5$; Laplace kernel, $\gamma = 6$; Rational Quadratic kernel, $\alpha = 0.25$; and Periodic kernel, period $p = 2$. Detailed formula and graphs for each kernel are provided in the appendix \ref {appendix:B}. }}{11}{table.caption.14}\protected@file@percent }
\newlabel{tab:kernel-examples}{{2.1}{11}{Visual comparison of common kernel functions and their effect on Gaussian process priors. Each column shows the kernel shape $k(x, x')$, samples from the corresponding GP prior, and a summary of the structure it imposes. All kernels were evaluated using a lengthscale parameter $\ell = 1$ (except where noted). For the Matern kernel, $\nu = 0.5$; Laplace kernel, $\gamma = 6$; Rational Quadratic kernel, $\alpha = 0.25$; and Periodic kernel, period $p = 2$. Detailed formula and graphs for each kernel are provided in the appendix \ref {appendix:B}}{table.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Sampling from the GP prior with mean 0 and covariance given by the RBF kernel. The first plot shows the effect of the lengthscale hyperparameter \(\ell \) on the GP prior.We fix the signal variance to 1. The second plot shows the effect of the signal variance hyperparameter \(\sigma _f^2\) on the GP prior.We fix the lengthscale to 0.5.}}{12}{figure.caption.15}\protected@file@percent }
\newlabel{fig: GPprior_hyperparams}{{2.2}{12}{Sampling from the GP prior with mean 0 and covariance given by the RBF kernel. The first plot shows the effect of the lengthscale hyperparameter \(\ell \) on the GP prior.We fix the signal variance to 1. The second plot shows the effect of the signal variance hyperparameter \(\sigma _f^2\) on the GP prior.We fix the lengthscale to 0.5}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Building the Posterior Distribution}{12}{subsection.2.1.4}\protected@file@percent }
\newlabel{sec: priortoposterior}{{2.1.4}{12}{Building the Posterior Distribution}{subsection.2.1.4}{}}
\newlabel{eq: predictive_mean}{{2.9b}{12}{Building the Posterior Distribution}{equation.2.9b}{}}
\newlabel{eq: predictive_variance}{{2.9c}{12}{Building the Posterior Distribution}{equation.2.9c}{}}
\newlabel{eq: predictive_dist}{{2.9}{12}{Building the Posterior Distribution}{equation.2.9c}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces 1D Gaussian Process Regression: Prior to Posterior. This sequence shows how the GP prior transforms into a posterior as more data points are added. The RBF kernel was used with hyper-parameters: $l = 1$, $\sigma ^2 = 0.5$.The black line represents the true function. The blue is the mean of each posterio distribution. The light blue shaded region is the credible interval and the grey lines are the samples drawn from each posterior/prior. }}{13}{figure.caption.16}\protected@file@percent }
\newlabel{fig: priortoposterior}{{2.3}{13}{1D Gaussian Process Regression: Prior to Posterior. This sequence shows how the GP prior transforms into a posterior as more data points are added. The RBF kernel was used with hyper-parameters: $l = 1$, $\sigma ^2 = 0.5$.The black line represents the true function. The blue is the mean of each posterio distribution. The light blue shaded region is the credible interval and the grey lines are the samples drawn from each posterior/prior}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Handling Noise in our Data}{13}{subsection.2.1.5}\protected@file@percent }
\newlabel{sec: Handlingnoise}{{2.1.5}{13}{Handling Noise in our Data}{subsection.2.1.5}{}}
\newlabel{eq: prior_distribution_noise}{{2.12}{14}{Handling Noise in our Data}{equation.2.12}{}}
\newlabel{eq: predictive_mean_noise}{{2.13b}{14}{Handling Noise in our Data}{equation.2.13b}{}}
\newlabel{eq: predictive_variance_noise}{{2.13c}{14}{Handling Noise in our Data}{equation.2.13c}{}}
\newlabel{eq: predictive_distribution_noise}{{2.13}{14}{Handling Noise in our Data}{equation.2.13c}{}}
\@writefile{toc}{\contentsline {subsubsection}{Homoscedastic Noise}{14}{section*.17}\protected@file@percent }
\newlabel{eq: prior_withnoise}{{2.14}{14}{Homoscedastic Noise}{equation.2.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Sampling from the GP prior with mean 0 and covariance given by the RBF kernel. The plot shows the effect of the noise hyperparameter \(\sigma _n^2\) on the GP prior.We fix the signal variance to 1 and length scale to 0.5.}}{15}{figure.caption.18}\protected@file@percent }
\newlabel{fig: kernel_noise}{{2.4}{15}{Sampling from the GP prior with mean 0 and covariance given by the RBF kernel. The plot shows the effect of the noise hyperparameter \(\sigma _n^2\) on the GP prior.We fix the signal variance to 1 and length scale to 0.5}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{Heteroscedastic Noise}{15}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Known Noise:}{15}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Learning Noise over the Input Space:}{15}{section*.21}\protected@file@percent }
\newlabel{eq:additive_kernel}{{2.15}{15}{Learning Noise over the Input Space:}{equation.2.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{Monte Carlo Sampling of Noise}{15}{section*.22}\protected@file@percent }
\citation{bible}
\@writefile{toc}{\contentsline {subsubsection}{Comparing Noise Models}{16}{section*.23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Samples drawn from a zero-mean Gaussian Process prior with varying noise assumptions. \textbf  {Left:} Homoscedastic noise, where a constant noise variance \(\sigma _n^2 = 0.5\) is added uniformly across all inputs. \textbf  {Middle:} Heteroscedastic noise, where individual noise variances \(\sigma _i^2\) are known and drawn from \(\mathcal  {N}(0, 0.5)\), resulting in a diagonal noise covariance. \textbf  {Right:} Monte Carlo sampling of noisy observations, where multiple noisy realizations are generated from \(\mathcal  {N}(f(x), \epsilon ^2)\)}}{16}{figure.caption.24}\protected@file@percent }
\newlabel{fig:noise_comparison}{{2.5}{16}{Samples drawn from a zero-mean Gaussian Process prior with varying noise assumptions. \textbf {Left:} Homoscedastic noise, where a constant noise variance \(\sigma _n^2 = 0.5\) is added uniformly across all inputs. \textbf {Middle:} Heteroscedastic noise, where individual noise variances \(\sigma _i^2\) are known and drawn from \(\mathcal {N}(0, 0.5)\), resulting in a diagonal noise covariance. \textbf {Right:} Monte Carlo sampling of noisy observations, where multiple noisy realizations are generated from \(\mathcal {N}(f(x), \epsilon ^2)\)}{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6}Hyper-parameters}{16}{subsection.2.1.6}\protected@file@percent }
\newlabel{sec: Hyper_parameters}{{2.1.6}{16}{Hyper-parameters}{subsection.2.1.6}{}}
\newlabel{eq: 5}{{2.16}{17}{Hyper-parameters}{equation.2.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces For a GPR with noise where we have the noise as a hyper-parameter we are forced to optimise a length \((\ell \)) hyper-parameter, a variance \(\sigma _f^2\) hyper-parameter and a noise \(\sigma _n^2\) hyper-parameter. Here we have kept one parameter constant on each graph and compared the log likelihood space varying the other two parameters. In the upper left panel the noise is set to $\sigma ^2_n= 0.1$, in the upper right panel the length is set at $ \ell =0.5$, in the lower left panel the variance is set at $\sigma ^2_f=1.5$. In the final panel we plot a 3-dimensional scatter plot and illustrate the point estimate given by the optimisation algorithm \texttt  {"fmin\_l\_bfgs\_b"} as the black dot. This point is located at $(\sigma _f^2 = 1.16, \ell = 0.109 \sigma _n^2 = 0.105)$.}}{17}{figure.caption.25}\protected@file@percent }
\newlabel{fig:Optimising_Hyperparams}{{2.6}{17}{For a GPR with noise where we have the noise as a hyper-parameter we are forced to optimise a length \((\ell \)) hyper-parameter, a variance \(\sigma _f^2\) hyper-parameter and a noise \(\sigma _n^2\) hyper-parameter. Here we have kept one parameter constant on each graph and compared the log likelihood space varying the other two parameters. In the upper left panel the noise is set to $\sigma ^2_n= 0.1$, in the upper right panel the length is set at $ \ell =0.5$, in the lower left panel the variance is set at $\sigma ^2_f=1.5$. In the final panel we plot a 3-dimensional scatter plot and illustrate the point estimate given by the optimisation algorithm \texttt {"fmin\_l\_bfgs\_b"} as the black dot. This point is located at $(\sigma _f^2 = 1.16, \ell = 0.109 \sigma _n^2 = 0.105)$}{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.7}Quantifying Hyper-parameter Uncertainty}{17}{subsection.2.1.7}\protected@file@percent }
\newlabel{sec: MCMC}{{2.1.7}{17}{Quantifying Hyper-parameter Uncertainty}{subsection.2.1.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces  Results from an MCMC run using a Gaussian Process Regression model with an RBF kernel and a WhiteKernel to model noise. We used the point estimates from Figure~\ref {fig:Optimising_Hyperparams} as the starting point for our samples. We sampled the parameter space using 12 walkers and taking 1000 steps with each walker. We discarded 100 of the initialed samples and thinned every 15 resulting in 720 final samples. (For more details on this implementation refer to the Appendix {\textcolor {red}{{Sean: MCMC Ref}} }.) Using these samples we plot the approximate posterior distributions for each hyperparameter. the vertical red, blue and green lines indicate peak, mean, and pointwise estimates respectively.}}{18}{figure.caption.26}\protected@file@percent }
\newlabel{fig:MCMCresults}{{2.7}{18}{Results from an MCMC run using a Gaussian Process Regression model with an RBF kernel and a WhiteKernel to model noise. We used the point estimates from Figure~\ref {fig:Optimising_Hyperparams} as the starting point for our samples. We sampled the parameter space using 12 walkers and taking 1000 steps with each walker. We discarded 100 of the initialed samples and thinned every 15 resulting in 720 final samples. (For more details on this implementation refer to the Appendix \Sean {MCMC Ref}.) Using these samples we plot the approximate posterior distributions for each hyperparameter. the vertical red, blue and green lines indicate peak, mean, and pointwise estimates respectively}{figure.caption.26}{}}
\citation{bible}
\citation{gprthesis}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.8}Multi-Dimensional GPR}{19}{subsection.2.1.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces  Visualisation of constructing a two-dimensional kernel by multiplying two one-dimensional RBF kernels, each operating on a separate input dimension. Both kernels use a length scale of \(\ell = 0.3\). The resulting product kernel models smooth interactions across both dimensions, and a sample drawn from the corresponding GP prior is shown.}}{19}{figure.caption.27}\protected@file@percent }
\newlabel{fig:2dkernels}{{2.8}{19}{Visualisation of constructing a two-dimensional kernel by multiplying two one-dimensional RBF kernels, each operating on a separate input dimension. Both kernels use a length scale of \(\ell = 0.3\). The resulting product kernel models smooth interactions across both dimensions, and a sample drawn from the corresponding GP prior is shown}{figure.caption.27}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methods}{20}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Method}{20}{section.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Method Flow Chart}}{20}{figure.caption.28}\protected@file@percent }
\newlabel{fig:flowchart}{{3.1}{20}{Method Flow Chart}{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}The Models}{20}{subsection.3.1.1}\protected@file@percent }
\newlabel{subsec:Models}{{3.1.1}{20}{The Models}{subsection.3.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Homoscedastic Noise Models}{20}{section*.29}\protected@file@percent }
\citation{metrics}
\@writefile{toc}{\contentsline {subsubsection}{Heteroscedastic Noise Models}{21}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Hybrid Model}{21}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Model Evaluation Metrics}{21}{subsection.3.1.2}\protected@file@percent }
\newlabel{sec:metrics}{{3.1.2}{21}{Model Evaluation Metrics}{subsection.3.1.2}{}}
\citation{metrics}
\citation{bible}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Summary of the Gaussian Process Regression models evaluated. ''All kernels'' refers to the RBF, Matern, Rational Quadratic, ExpSine Squared and the Laplace kernels discussed in Section~\ref {sec: Kernels}. In the ''All Kernels'' case the method was ran for every kernel.}}{22}{table.caption.32}\protected@file@percent }
\newlabel{tab:model_summary}{{3.1}{22}{Summary of the Gaussian Process Regression models evaluated. ''All kernels'' refers to the RBF, Matern, Rational Quadratic, ExpSine Squared and the Laplace kernels discussed in Section~\ref {sec: Kernels}. In the ''All Kernels'' case the method was ran for every kernel}{table.caption.32}{}}
\@writefile{toc}{\contentsline {subsubsection}{AEE Metrics}{22}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Correlation Metrics}{22}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Model Training, Testing and Comparisons}{23}{subsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Cross Validation on all Model Types}{23}{section*.35}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Final Testing}{23}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Implementation Details}{23}{section*.37}\protected@file@percent }
\newlabel{subsubsec:implementdetails}{{3.1.3}{23}{Implementation Details}{section*.37}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{24}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Results}{24}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Cross-Validation Performance}{24}{subsection.4.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Comparison of the average performance of each model type and for each kernel across all 10 folds of the training/validation data for each metric. Note: The \texttt  {combinedkernel} approach is excluded here for clarity, but is included in all subsequent evaluations.}}{24}{figure.caption.38}\protected@file@percent }
\newlabel{fig:broad_comparison}{{4.1}{24}{Comparison of the average performance of each model type and for each kernel across all 10 folds of the training/validation data for each metric. Note: The \texttt {combinedkernel} approach is excluded here for clarity, but is included in all subsequent evaluations}{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Left: Heatmap showing each model’s rank across the evaluation metrics. The x-axis lists models according to their ranking in Table~\ref {tab:rankingtable}, and the y-axis shows the metrics. The color bar represents rank (blue indicates better rank, red indicates worse). Middle: Dendrogram showing hierarchical clustering of metrics based on how similarly they rank models. The vertical axis denotes correlation distance—smaller values indicate higher agreement between metric rankings. Right: Scatter plot of all models with \(R^2\) on the x-axis, FOM on the y-axis, and MAE represented by the color of each point. Models are indexed by their rank from Table~\ref {tab:rankingtable}.}}{25}{figure.caption.39}\protected@file@percent }
\newlabel{fig:CV_sidebyside}{{4.2}{25}{Left: Heatmap showing each model’s rank across the evaluation metrics. The x-axis lists models according to their ranking in Table~\ref {tab:rankingtable}, and the y-axis shows the metrics. The color bar represents rank (blue indicates better rank, red indicates worse). Middle: Dendrogram showing hierarchical clustering of metrics based on how similarly they rank models. The vertical axis denotes correlation distance—smaller values indicate higher agreement between metric rankings. Right: Scatter plot of all models with \(R^2\) on the x-axis, FOM on the y-axis, and MAE represented by the color of each point. Models are indexed by their rank from Table~\ref {tab:rankingtable}}{figure.caption.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Training on 90\% of Data}{25}{subsection.4.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Left: Heatmap of top 8 model rankings by metric indexed by ranking. Right: Scatter of FOM against \(R^2\) with the colour bar representing the RMSE. The Rankings are given in Table~\ref {tab:finalmadelsrankingtable}.}}{26}{figure.caption.40}\protected@file@percent }
\newlabel{fig:comparing_metrics}{{4.3}{26}{Left: Heatmap of top 8 model rankings by metric indexed by ranking. Right: Scatter of FOM against \(R^2\) with the colour bar representing the RMSE. The Rankings are given in Table~\ref {tab:finalmadelsrankingtable}}{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Comparing Cross-cuts of best models. See text for details. Recall that $z=1\ (z=-1)$ corresponds to mass ratio of $q=1\ (q=1/4)$, and similarly $w=0.25$ corresponds to $M_\text  {tot}=37.5M_\odot $.}}{27}{figure.caption.41}\protected@file@percent }
\newlabel{fig:crosscuts_bestmodels}{{4.4}{27}{Comparing Cross-cuts of best models. See text for details. Recall that $z=1\ (z=-1)$ corresponds to mass ratio of $q=1\ (q=1/4)$, and similarly $w=0.25$ corresponds to $M_\text {tot}=37.5M_\odot $}{figure.caption.41}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces This table contains the optimized hyperparameters for the final 8 GPR models. $\sigma _{f,1}^2$ and $\sigma _{f,2}^2$ represent the constant factor the each kernel is multiplied by and is referred to as the signal variance since it controls the amplitute of the resulting model. The Length scales are the corresponding length scales for each dimension and then $\sigma _n^2$ is the optimised noise hyper-parameter introduced by the \textsc  {WhiteKernel}. Mat and Lap refer to the Matern and Laplacian kernel.}}{27}{table.caption.42}\protected@file@percent }
\newlabel{tab:final_gpr_hyperparams}{{4.1}{27}{This table contains the optimized hyperparameters for the final 8 GPR models. $\sigma _{f,1}^2$ and $\sigma _{f,2}^2$ represent the constant factor the each kernel is multiplied by and is referred to as the signal variance since it controls the amplitute of the resulting model. The Length scales are the corresponding length scales for each dimension and then $\sigma _n^2$ is the optimised noise hyper-parameter introduced by the \textsc {WhiteKernel}. Mat and Lap refer to the Matern and Laplacian kernel}{table.caption.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Hyper-parameter Uncertainty}{27}{subsection.4.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The hyper-parameter posterior of the best \texttt  {RBFMatern} model. The model hyper-parameters from Table~\ref {tab:final_gpr_hyperparams} initialise each of the walkers (4 walkers for every extra parameter so 40 walkers). For each walker we generate 300 samples and use a burnin of 100 and a thin of 15 resulting in a total of 560 samples used to build this distribution. The resulting parameter distributions are plotted. The vertical red, blue and green lines indicate peak, mean, and pointwise estimates respectively.}}{28}{figure.caption.43}\protected@file@percent }
\newlabel{fig:MCMCRBFMatern}{{4.5}{28}{The hyper-parameter posterior of the best \texttt {RBFMatern} model. The model hyper-parameters from Table~\ref {tab:final_gpr_hyperparams} initialise each of the walkers (4 walkers for every extra parameter so 40 walkers). For each walker we generate 300 samples and use a burnin of 100 and a thin of 15 resulting in a total of 560 samples used to build this distribution. The resulting parameter distributions are plotted. The vertical red, blue and green lines indicate peak, mean, and pointwise estimates respectively}{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Plotting the marginalised posterior distribution over the hyper-parameters with the point wise \texttt  {RBFMatern} model. We again plot cross-cuts of our data, taking \(z = (1,0,-1)\) which corresponds to the mass ratio of \(q = (1,0.404,0.25)\) and \(w = 0.25\) which corresponds to \(M_{\text  {tot}} = 37.5M_\odot \). We are marginalising over the distributions in Figure~\ref {fig:MCMCRBFMatern}. Since marginalising is computationally expensive of our 300 samples for 40 walkers we discard the first 200 samples and thin every 10 sample. By doing this we marginalise over 400 converged MCMC samples.}}{29}{figure.caption.44}\protected@file@percent }
\newlabel{fig:MCMCvsmarginalised}{{4.6}{29}{Plotting the marginalised posterior distribution over the hyper-parameters with the point wise \texttt {RBFMatern} model. We again plot cross-cuts of our data, taking \(z = (1,0,-1)\) which corresponds to the mass ratio of \(q = (1,0.404,0.25)\) and \(w = 0.25\) which corresponds to \(M_{\text {tot}} = 37.5M_\odot \). We are marginalising over the distributions in Figure~\ref {fig:MCMCRBFMatern}. Since marginalising is computationally expensive of our 300 samples for 40 walkers we discard the first 200 samples and thin every 10 sample. By doing this we marginalise over 400 converged MCMC samples}{figure.caption.44}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Comparison of regression performance metrics for the RBFMat model using a pointwise (MAP) solution versus the MCMC marginalised model. The MCMC model reflects increased uncertainty, resulting in reduced fit scores but a more robust and probabilistically faithful representation.}}{29}{table.caption.45}\protected@file@percent }
\newlabel{tab:pointwise_vs_mcmc}{{4.2}{29}{Comparison of regression performance metrics for the RBFMat model using a pointwise (MAP) solution versus the MCMC marginalised model. The MCMC model reflects increased uncertainty, resulting in reduced fit scores but a more robust and probabilistically faithful representation}{table.caption.45}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{30}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Conclusion/Discussion}{30}{section.5.1}\protected@file@percent }
\bibdata{references}
\bibcite{linearcombination}{1}
\bibcite{intoGRSarp}{2}
\bibcite{Bayesianapproach}{3}
\bibcite{NRsimulation}{4}
\bibcite{kernelcookbook}{5}
\bibcite{gprthesis}{6}
\bibcite{NRfitMT}{7}
\bibcite{Ogpaper}{8}
\bibcite{GRbook}{9}
\bibcite{mismatch}{10}
\bibcite{NRfitMP}{11}
\bibcite{bestNRfitS}{12}
\bibcite{bible}{13}
\bibcite{NRsurrogate}{14}
\bibcite{metrics}{15}
\@writefile{toc}{\contentsline {section}{\numberline {.1}Derivation of Predictive Distribution}{34}{section.Alph0.1}\protected@file@percent }
\newlabel{appendix:A}{{.1}{34}{Derivation of Predictive Distribution}{section.Alph0.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {.1.1}Kernel Formulas}{34}{subsection.Alph0.1.1}\protected@file@percent }
\newlabel{appendix:B}{{.1.1}{34}{Kernel Formulas}{subsection.Alph0.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {.1.2}Graphs of 4d finalist GPs}{36}{subsection.Alph0.1.2}\protected@file@percent }
\newlabel{appendix:C}{{.1.2}{36}{Graphs of 4d finalist GPs}{subsection.Alph0.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textcolor {orange}{\texttt  {TODO: Improve caption}} All 8 gps with cutting their y-axis}}{36}{figure.caption.53}\protected@file@percent }
\newlabel{fig:best8_ycuts}{{1}{36}{\todo {Improve caption} All 8 gps with cutting their y-axis}{figure.caption.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces All 8 gps with cutting their x-axis}}{37}{figure.caption.54}\protected@file@percent }
\newlabel{fig:best8_xcuts}{{2}{37}{All 8 gps with cutting their x-axis}{figure.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Examining my chosen model RBF Matern kernel}}{38}{figure.caption.55}\protected@file@percent }
\newlabel{fig:RBF_Matern_xcuts}{{3}{38}{Examining my chosen model RBF Matern kernel}{figure.caption.55}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {.1.3}Model Evaluation Table and graphs}{38}{subsection.Alph0.1.3}\protected@file@percent }
\newlabel{appendix:D}{{.1.3}{38}{Model Evaluation Table and graphs}{subsection.Alph0.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Seeing how the best models performed over different clusters}}{38}{figure.caption.56}\protected@file@percent }
\newlabel{fig:boxplots}{{4}{38}{Seeing how the best models performed over different clusters}{figure.caption.56}{}}
\citation{bible}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Final Model Rankings after training on 90\% and testing on 10\%}}{39}{table.caption.57}\protected@file@percent }
\newlabel{tab:finalmadelsrankingtable}{{1}{39}{Final Model Rankings after training on 90\% and testing on 10\%}{table.caption.57}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces All 32 Model Rankings from CV}}{39}{table.caption.58}\protected@file@percent }
\newlabel{tab:rankingtable}{{2}{39}{All 32 Model Rankings from CV}{table.caption.58}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {.1.4}Bin}{39}{subsection.Alph0.1.4}\protected@file@percent }
\newlabel{appendix:bin}{{.1.4}{39}{Bin}{subsection.Alph0.1.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Comparison of different performance metrics used in evaluating models. RMSE, \( R^2 \), FOM, and the Pearson Coefficient are included. MAE is similar to RMSE but without squaring errors. Adjusted \( R^2 \) accounts for the number of predictors and is slightly modified from \( R^2 \). The actual metrics for each graph are: RMSE = 0.2, \( R^2 \) = 0.6, FOM = 1.09, Pearson correlation = 0.8.}}{40}{table.caption.59}\protected@file@percent }
\newlabel{tab:metrics-comparison}{{3}{40}{Comparison of different performance metrics used in evaluating models. RMSE, \( R^2 \), FOM, and the Pearson Coefficient are included. MAE is similar to RMSE but without squaring errors. Adjusted \( R^2 \) accounts for the number of predictors and is slightly modified from \( R^2 \). The actual metrics for each graph are: RMSE = 0.2, \( R^2 \) = 0.6, FOM = 1.09, Pearson correlation = 0.8}{table.caption.59}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {.1.5}MCMC details}{41}{subsection.Alph0.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Overview of the MCMC sampling procedure for Gaussian Process hyperparameter inference. This pipeline samples from the posterior \( p(\theta \mid \mathbf  {y}, X) \) using a Metropolis-Hastings Gaussian proposal and an ensemble of walkers.}}{41}{figure.caption.61}\protected@file@percent }
\newlabel{fig:MCMC flowchart}{{5}{41}{Overview of the MCMC sampling procedure for Gaussian Process hyperparameter inference. This pipeline samples from the posterior \( p(\theta \mid \mathbf {y}, X) \) using a Metropolis-Hastings Gaussian proposal and an ensemble of walkers}{figure.caption.61}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {.1.6}Noise modeling using Monte Carlo Sampling}{41}{subsection.Alph0.1.6}\protected@file@percent }
\newlabel{appendix:monte_carlo}{{.1.6}{41}{Noise modeling using Monte Carlo Sampling}{subsection.Alph0.1.6}{}}
\gdef \@abspage@last{51}
