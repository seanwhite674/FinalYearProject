\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{acm}
\citation{owen1995template}
\citation{hoy2024multi}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Background}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Gaussian Process Regression \textbf  {GPR} Background}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The Prior Distribution}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Adding Data: Prior to Posterior}{3}{subsection.2.2}\protected@file@percent }
\newlabel{sec: priortoposterior}{{2.2}{3}{Adding Data: Prior to Posterior}{subsection.2.2}{}}
\newlabel{eq: 1}{{1}{3}{Adding Data: Prior to Posterior}{equation.2.1}{}}
\citation{rasmussen2006gaussian}
\citation{duvenaud2014kernel}
\newlabel{eq: 2}{{2}{4}{Adding Data: Prior to Posterior}{equation.2.2}{}}
\newlabel{eq: 3}{{3}{4}{Adding Data: Prior to Posterior}{equation.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces 1D Gaussian Process Regression: Prior to Posterior. This sequence shows how the GP prior transforms into a posterior as more data points are added. The RBF kernel was used with hyper-parameters: $l = 1$, $\sigma ^2 = 0.5$.\relax }}{4}{figure.caption.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Kernel Functions}{4}{subsection.2.3}\protected@file@percent }
\newlabel{sec: Kernels}{{2.3}{4}{Kernel Functions}{subsection.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Stationary Kernels:}{4}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Non-Stationary Kernels:}{5}{section*.11}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  Visual comparison of common kernel functions and their effect on Gaussian process priors. Each column shows the kernel shape $k(x, x')$, samples from the corresponding GP prior, and a summary of the structure it imposes. All kernels were evaluated using a lengthscale parameter $\ell = 1$ (except where noted). For the Matern kernel, $\nu = 0.5$; Laplace kernel, $\gamma = 6$; Rational Quadratic kernel, $\alpha = 0.25$; and Periodic kernel, period $p = 2$. \relax }}{6}{table.caption.14}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:kernel-examples}{{1}{6}{Visual comparison of common kernel functions and their effect on Gaussian process priors. Each column shows the kernel shape $k(x, x')$, samples from the corresponding GP prior, and a summary of the structure it imposes. All kernels were evaluated using a lengthscale parameter $\ell = 1$ (except where noted). For the Matern kernel, $\nu = 0.5$; Laplace kernel, $\gamma = 6$; Rational Quadratic kernel, $\alpha = 0.25$; and Periodic kernel, period $p = 2$. \relax }{table.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Handling Noise in our Data}{6}{subsection.2.4}\protected@file@percent }
\newlabel{sec: Handlingnoise}{{2.4}{6}{Handling Noise in our Data}{subsection.2.4}{}}
\newlabel{eq: 4}{{4}{6}{Handling Noise in our Data}{equation.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Left: A white kernel is used to add the noise as a hyper-parameter which is optimised (homoscedastic noise). Middle: We update our covariance function with the true variance at the training points (heteroscedastic noise). Right: Result of Monte Carlo sampling of normally distributed noise at each training point (heteroscedastic noise). \relax }}{7}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Hyper-parameters}{7}{subsection.2.5}\protected@file@percent }
\citation{rasmussen2006gaussian}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces This figure outlines the effect of different RBF kernel hyperparameters on Gaussian Process Regression predictions.For each graph the mean is plotted along with the 95\% credible region. Each hyper-parameter is set at $\ell =0.109$, $\sigma _f^2 =1.2$ and $\sigma _n^2 = 0.15$ unless explicitly changed\relax }}{8}{figure.caption.16}\protected@file@percent }
\newlabel{figure: Effect of Hyper-parameters}{{3}{8}{This figure outlines the effect of different RBF kernel hyperparameters on Gaussian Process Regression predictions.For each graph the mean is plotted along with the 95\% credible region. Each hyper-parameter is set at $\ell =0.109$, $\sigma _f^2 =1.2$ and $\sigma _n^2 = 0.15$ unless explicitly changed\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Learning Hyperparameters by Maximising the Log Marginal Likelihood}{9}{subsubsection.2.5.1}\protected@file@percent }
\newlabel{sec: hyperparam optimisation}{{2.5.1}{9}{Learning Hyperparameters by Maximising the Log Marginal Likelihood}{subsubsection.2.5.1}{}}
\newlabel{eq: 5}{{5}{9}{Learning Hyperparameters by Maximising the Log Marginal Likelihood}{equation.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces For a GPR with noise we are forced to optimise a length hyper-parameter, a variance hyper-parameter and a noise hyper-parameter. Here we have kept one parameter constant on each graph and compared the log likelihood space varying the other two parameters. In graph one the noise is set at 0.1, in graph 2 the length is set at 0.5, in graph 3 the variance is set at 1.5. In the final graph we plot a 3-dimensional scatter plot and illustrate the point estimate given by the optimisation algorithm as the black dot.This point is located at $(\sigma ^2 = 1.16, l = 0.109$ noise$ = 0.105)$\relax }}{10}{figure.caption.17}\protected@file@percent }
\newlabel{figure: Optimising Hyper-params}{{4}{10}{For a GPR with noise we are forced to optimise a length hyper-parameter, a variance hyper-parameter and a noise hyper-parameter. Here we have kept one parameter constant on each graph and compared the log likelihood space varying the other two parameters. In graph one the noise is set at 0.1, in graph 2 the length is set at 0.5, in graph 3 the variance is set at 1.5. In the final graph we plot a 3-dimensional scatter plot and illustrate the point estimate given by the optimisation algorithm as the black dot.This point is located at $(\sigma ^2 = 1.16, l = 0.109$ noise$ = 0.105)$\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Cross Validation}{10}{subsubsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Monte Carlo Markov Chain (MCMC): A Posterior over Hyperparameters}{10}{subsection.2.6}\protected@file@percent }
\newlabel{sec: MCMC}{{2.6}{10}{Monte Carlo Markov Chain (MCMC): A Posterior over Hyperparameters}{subsection.2.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Why MCMC and the Mathematics Behind It}{10}{subsubsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}Implementing MCMC}{11}{subsubsection.2.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Overview of the MCMC sampling procedure for Gaussian Process hyperparameter inference. This pipeline samples from the posterior \( p(\theta \mid \mathbf  {y}, X) \) using a Metropolis-Hastings Gaussian proposal and an ensemble of walkers.\relax }}{11}{figure.caption.18}\protected@file@percent }
\newlabel{fig:MCMC flowchart}{{5}{11}{Overview of the MCMC sampling procedure for Gaussian Process hyperparameter inference. This pipeline samples from the posterior \( p(\theta \mid \mathbf {y}, X) \) using a Metropolis-Hastings Gaussian proposal and an ensemble of walkers.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces This MCMC is run using a gpr with a RBF Kernel with White Kernel for noise.Left: A plot of the space sampled by each walker in my MCMC run (burnin=100 and thin=15). \hskip 1em\relax Right: A plot of the distribution of the hyper-parameters built using a Gaussian KDE from the MCMC samples.\relax }}{12}{figure.caption.19}\protected@file@percent }
\newlabel{fig:MCMCresults}{{6}{12}{This MCMC is run using a gpr with a RBF Kernel with White Kernel for noise.Left: A plot of the space sampled by each walker in my MCMC run (burnin=100 and thin=15). \quad Right: A plot of the distribution of the hyper-parameters built using a Gaussian KDE from the MCMC samples.\relax }{figure.caption.19}{}}
\citation{rasmussen2006gaussian}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Left: This is the GPR plotted with the mean hyper-parameters Middle: This is the GPR plotted with the peak hyper-parameters Right: This is the full predictive distribution over all the hyper-parameter samples \relax }}{13}{figure.caption.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Metrics for testing model}{13}{subsection.2.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison of different performance metrics used in evaluating models. RMSE, \( R^2 \), FOM, and the Pearson Coefficient are included. MAE is similar to RMSE but without squaring errors. Adjusted \( R^2 \) accounts for the number of predictors and is slightly modified from \( R^2 \). The actual metrics for each graph are: RMSE = 0.2, \( R^2 \) = 0.6, FOM = 1.09, Pearson correlation = 0.8.\relax }}{13}{table.caption.21}\protected@file@percent }
\newlabel{tab:metrics-comparison}{{2}{13}{Comparison of different performance metrics used in evaluating models. RMSE, \( R^2 \), FOM, and the Pearson Coefficient are included. MAE is similar to RMSE but without squaring errors. Adjusted \( R^2 \) accounts for the number of predictors and is slightly modified from \( R^2 \). The actual metrics for each graph are: RMSE = 0.2, \( R^2 \) = 0.6, FOM = 1.09, Pearson correlation = 0.8.\relax }{table.caption.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Multi-Dimensional Gaussian Process Regression}{13}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Method}{14}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces My Process Flowchart\relax }}{14}{figure.caption.22}\protected@file@percent }
\newlabel{fig:flowchart}{{8}{14}{My Process Flowchart\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Method Over-view}{14}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}My data 4D - 7D}{14}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Training and Testing Protocol}{14}{subsection.4.3}\protected@file@percent }
\bibdata{references}
\bibcite{duvenaud2014kernel}{1}
\bibcite{hoy2024multi}{2}
\bibcite{owen1995template}{3}
\bibcite{rasmussen2006gaussian}{4}
\@writefile{toc}{\contentsline {section}{\numberline {5}Different Models}{15}{section.5}\protected@file@percent }
