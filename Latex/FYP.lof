\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Motivating NR Method Figure}}{5}{figure.caption.8}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces Visualising a binary black hole system. The two black holes with masses \(M_1\) and \(M_2\) orbit each other in a quasi-circular orbit, each with spin vectors \(\mathbf {S}_1\) and \(\mathbf {S}_2\). The total spin is \(\mathbf {S}_{\text {total}} = \mathbf {S}_1+\mathbf {S}_2\) and \(\mathbf {L}\) is the orbital angular momentum. The spin projections \( \chi _{\parallel } \) and \( \chi _{\perp } \) are the components of the total spin that are parallel and perpendicular to \( \mathbf {L} \), respectively.}}{6}{figure.caption.9}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces Visualisation of the input data across reduced dimensions. Left: A 3D scatter plot of all data points for fixed total mass \(w = 0.25\) ($M_\text {tot}=37.5 M_\odot $). Centre: A 2D slice of the data at rescaled symmetric mass ratio \(z = 0\) ($q=0.404$), interpolated over spin components. Right: A 1D cut through the data at \(x = 0.8\) showing variation across \(y\). When using raw data, interpolation is needed between samples, but GPR provides an analytic model that can be directly evaluated without interpolation. }}{7}{figure.caption.10}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Sampling from the GP prior with zero mean and an RBF kernel ($\ell = 0.5$, $\sigma _f^2 = 1$). The first plot shows three sample functions drawn from the prior distribution. The second plot visualizes the covariance matrix as a heatmap, revealing the strength of correlations between inputs. The final three subplots display joint distributions between selected input pairs. These distributions are multivariate distributions with mean zero and covariance matrix defined by a \(2 \times 2\) where the diagonals are 1 and the off diagonals are the correlation between the selected input points. The contours are drawn at multiples of \(0.5 \sigma \) indicating confidence regions for each distribution}}{10}{figure.caption.13}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Sampling from the GP prior with mean 0 and covariance given by the RBF kernel. The first plot shows the effect of the lengthscale hyperparameter \(\ell \) on the GP prior.We fix the signal variance to 1. The second plot shows the effect of the signal variance hyperparameter \(\sigma _f^2\) on the GP prior.We fix the lengthscale to 0.5.}}{12}{figure.caption.15}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces 1D Gaussian Process Regression: Prior to Posterior. This sequence shows how the GP prior transforms into a posterior as more data points are added. The RBF kernel was used with hyper-parameters: $l = 1$, $\sigma ^2 = 0.5$.The black line represents the true function. The blue is the mean of each posterio distribution. The light blue shaded region is the credible interval and the grey lines are the samples drawn from each posterior/prior. }}{13}{figure.caption.16}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Sampling from the GP prior with mean 0 and covariance given by the RBF kernel. The plot shows the effect of the noise hyperparameter \(\sigma _n^2\) on the GP prior.We fix the signal variance to 1 and length scale to 0.5.}}{15}{figure.caption.18}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Samples drawn from a zero-mean Gaussian Process prior with varying noise assumptions. \textbf {Left:} Homoscedastic noise, where a constant noise variance \(\sigma _n^2 = 0.5\) is added uniformly across all inputs. \textbf {Middle:} Heteroscedastic noise, where individual noise variances \(\sigma _i^2\) are known and drawn from \(\mathcal {N}(0, 0.5)\), resulting in a diagonal noise covariance. \textbf {Right:} Monte Carlo sampling of noisy observations, where multiple noisy realizations are generated from \(\mathcal {N}(f(x), \epsilon ^2)\)}}{16}{figure.caption.24}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces For a GPR with noise where we have the noise as a hyper-parameter we are forced to optimise a length \((\ell \)) hyper-parameter, a variance \(\sigma _f^2\) hyper-parameter and a noise \(\sigma _n^2\) hyper-parameter. Here we have kept one parameter constant on each graph and compared the log likelihood space varying the other two parameters. In the upper left panel the noise is set to $\sigma ^2_n= 0.1$, in the upper right panel the length is set at $ \ell =0.5$, in the lower left panel the variance is set at $\sigma ^2_f=1.5$. In the final panel we plot a 3-dimensional scatter plot and illustrate the point estimate given by the optimisation algorithm \texttt {"fmin\_l\_bfgs\_b"} as the black dot. This point is located at $(\sigma _f^2 = 1.16, \ell = 0.109 \sigma _n^2 = 0.105)$.}}{17}{figure.caption.25}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces Results from an MCMC run using a Gaussian Process Regression model with an RBF kernel and a WhiteKernel to model noise. We used the point estimates from Figure~\ref {fig:Optimising_Hyperparams} as the starting point for our samples. We sampled the parameter space using 12 walkers and taking 1000 steps with each walker. We discarded 100 of the initialed samples and thinned every 15 resulting in 720 final samples. (For more details on this implementation refer to the Appendix {\textcolor {red}{{Sean: MCMC Ref}} }.) Using these samples we plot the approximate posterior distributions for each hyperparameter. the vertical red, blue and green lines indicate peak, mean, and pointwise estimates respectively.}}{18}{figure.caption.26}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces Visualisation of constructing a two-dimensional kernel by multiplying two one-dimensional RBF kernels, each operating on a separate input dimension. Both kernels use a length scale of \(\ell = 0.3\). The resulting product kernel models smooth interactions across both dimensions, and a sample drawn from the corresponding GP prior is shown.}}{19}{figure.caption.27}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Method Flow Chart}}{20}{figure.caption.28}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Comparison of the average performance of each model type and for each kernel across all 10 folds of the training/validation data for each metric. Note: The \texttt {combinedkernel} approach is excluded here for clarity, but is included in all subsequent evaluations.}}{24}{figure.caption.38}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Left: Heatmap showing each model’s rank across the evaluation metrics. The x-axis lists models according to their ranking in Table~\ref {tab:rankingtable}, and the y-axis shows the metrics. The color bar represents rank (blue indicates better rank, red indicates worse). Middle: Dendrogram showing hierarchical clustering of metrics based on how similarly they rank models. The vertical axis denotes correlation distance—smaller values indicate higher agreement between metric rankings. Right: Scatter plot of all models with \(R^2\) on the x-axis, FOM on the y-axis, and MAE represented by the color of each point. Models are indexed by their rank from Table~\ref {tab:rankingtable}.}}{25}{figure.caption.39}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Left: Heatmap of top 8 model rankings by metric indexed by ranking. Right: Scatter of FOM against \(R^2\) with the colour bar representing the RMSE. The Rankings are given in Table~\ref {tab:finalmadelsrankingtable}.}}{26}{figure.caption.40}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces Comparing Cross-cuts of best models. See text for details. Recall that $z=1\ (z=-1)$ corresponds to mass ratio of $q=1\ (q=1/4)$, and similarly $w=0.25$ corresponds to $M_\text {tot}=37.5M_\odot $.}}{27}{figure.caption.41}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces The hyper-parameter posterior of the best \texttt {RBFMatern} model. The model hyper-parameters from Table~\ref {tab:final_gpr_hyperparams} initialise each of the walkers (4 walkers for every extra parameter so 40 walkers). For each walker we generate 300 samples and use a burnin of 100 and a thin of 15 resulting in a total of 560 samples used to build this distribution. The resulting parameter distributions are plotted. The vertical red, blue and green lines indicate peak, mean, and pointwise estimates respectively.}}{28}{figure.caption.43}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces Plotting the marginalised posterior distribution over the hyper-parameters with the point wise \texttt {RBFMatern} model. We again plot cross-cuts of our data, taking \(z = (1,0,-1)\) which corresponds to the mass ratio of \(q = (1,0.404,0.25)\) and \(w = 0.25\) which corresponds to \(M_{\text {tot}} = 37.5M_\odot \). We are marginalising over the distributions in Figure~\ref {fig:MCMCRBFMatern}. Since marginalising is computationally expensive of our 300 samples for 40 walkers we discard the first 200 samples and thin every 10 sample. By doing this we marginalise over 400 converged MCMC samples.}}{29}{figure.caption.44}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1}{\ignorespaces \textcolor {orange}{\texttt {TODO: Improve caption}} All 8 gps with cutting their y-axis}}{36}{figure.caption.53}%
\contentsline {figure}{\numberline {2}{\ignorespaces All 8 gps with cutting their x-axis}}{37}{figure.caption.54}%
\contentsline {figure}{\numberline {3}{\ignorespaces Examining my chosen model RBF Matern kernel}}{38}{figure.caption.55}%
\contentsline {figure}{\numberline {4}{\ignorespaces Seeing how the best models performed over different clusters}}{38}{figure.caption.56}%
\contentsline {figure}{\numberline {5}{\ignorespaces Overview of the MCMC sampling procedure for Gaussian Process hyperparameter inference. This pipeline samples from the posterior \( p(\theta \mid \mathbf {y}, X) \) using a Metropolis-Hastings Gaussian proposal and an ensemble of walkers.}}{41}{figure.caption.61}%
